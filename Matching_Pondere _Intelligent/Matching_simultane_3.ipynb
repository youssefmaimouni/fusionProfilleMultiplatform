{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d9f51d9-bd06-4791-896f-c569c913a77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ √âTAPE 3 : Matching Triplet Simultan√© (GitHub + LinkedIn + Twitter)\n",
      "üìä Profils charg√©s : 3770 GitHub, 4276 LinkedIn, 3353 Twitter\n",
      "‚úÖ 388 triplets complets identifi√©s\n",
      "‚úÖ R√©sultats sauvegard√©s dans 'output/unified_triplets.json'\n",
      "‚û°Ô∏è Pr√™t pour analyse fine (ex: localisation Marocaine)\n"
     ]
    }
   ],
   "source": [
    "# step3_triplet_matching.py\n",
    "\"\"\"\n",
    "√âTAPE 3 : Matching Triplet Simultan√© (GitHub + LinkedIn + Twitter)\n",
    "\n",
    "Objectif : Trouver des groupes de 3 profils (un par plateforme) appartenant\n",
    "√† la m√™me personne, en √©valuant les trois ensemble.\n",
    "\n",
    "Fonctionnalit√©s :\n",
    "- Score global pond√©r√© : email (0.30), fullName (0.25), repo/about (0.30), username (0.10), bio (0.05)\n",
    "- Support des liens externes (ex: GitHub bio mentionne Twitter/LinkedIn)\n",
    "- Matching 1:1:1 strict (pas de chevauchement)\n",
    "- Sortie JSON valide (conversion des types NumPy)\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "def convert_numpy_types(obj):\n",
    "    \"\"\"Convertit r√©cursivement les types NumPy en types natifs Python pour JSON.\"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_numpy_types(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, (list, tuple)):\n",
    "        return [convert_numpy_types(x) for x in obj]\n",
    "    elif isinstance(obj, (np.integer, np.floating)):\n",
    "        return obj.item()\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "def normalize_email(email):\n",
    "    if not email or \"@\" not in str(email):\n",
    "        return None\n",
    "    return str(email).strip().lower()\n",
    "\n",
    "def cosine_sim(u, v):\n",
    "    norm_u = np.linalg.norm(u)\n",
    "    norm_v = np.linalg.norm(v)\n",
    "    if norm_u == 0 or norm_v == 0:\n",
    "        return 0.0\n",
    "    return np.dot(u, v) / (norm_u * norm_v + 1e-8)\n",
    "\n",
    "def extract_first_name(name):\n",
    "    if not name or not isinstance(name, str):\n",
    "        return \"\"\n",
    "    parts = name.strip().split()\n",
    "    return parts[0].lower() if parts else \"\"\n",
    "\n",
    "def main():\n",
    "    print(\"üöÄ √âTAPE 3 : Matching Triplet Simultan√© (GitHub + LinkedIn + Twitter)\")\n",
    "    output_dir = Path(\"output\")\n",
    "\n",
    "    # Charger les profils\n",
    "    with open(output_dir / \"profiles_metadata.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        profiles = json.load(f)\n",
    "\n",
    "    # Charger les embeddings\n",
    "    embeddings = {}\n",
    "    for field in [\"fullName\", \"username\", \"bio\", \"repo_descriptions\", \"headline\"]:\n",
    "        embeddings[field] = np.load(output_dir / f\"{field}_embeddings.npy\")\n",
    "\n",
    "    # Indexer par plateforme + ajouter identifiant global\n",
    "    github_profiles = []\n",
    "    linkedin_profiles = []\n",
    "    twitter_profiles = []\n",
    "    profile_to_index = {}\n",
    "\n",
    "    for idx, p in enumerate(profiles):\n",
    "        p[\"email_norm\"] = normalize_email(p.get(\"email\"))\n",
    "        p[\"first_name\"] = extract_first_name(p.get(\"fullName\", \"\"))\n",
    "        p[\"_global_index\"] = idx\n",
    "        profile_to_index[idx] = p\n",
    "\n",
    "        plat = p[\"platform\"].lower()\n",
    "        if plat == \"github\":\n",
    "            github_profiles.append(p)\n",
    "        elif plat == \"linkedin\":\n",
    "            linkedin_profiles.append(p)\n",
    "        elif plat == \"twitter\":\n",
    "            twitter_profiles.append(p)\n",
    "\n",
    "    print(f\"üìä Profils charg√©s : {len(github_profiles)} GitHub, \"\n",
    "          f\"{len(linkedin_profiles)} LinkedIn, {len(twitter_profiles)} Twitter\")\n",
    "\n",
    "    # Indexation par attribut (liste de dictionnaires)\n",
    "    email_to_profiles = defaultdict(list)\n",
    "    username_to_profiles = defaultdict(list)\n",
    "    first_name_to_profiles = defaultdict(list)\n",
    "\n",
    "    for p in profiles:\n",
    "        if p[\"email_norm\"]:\n",
    "            email_to_profiles[p[\"email_norm\"]].append(p)\n",
    "        if p.get(\"username\"):\n",
    "            username_to_profiles[p[\"username\"].lower()].append(p)\n",
    "        if p[\"first_name\"]:\n",
    "            first_name_to_profiles[p[\"first_name\"]].append(p)\n",
    "\n",
    "    # G√©n√©rer les triplets candidats\n",
    "    triplet_candidates = []\n",
    "\n",
    "    for gh in github_profiles:\n",
    "        gh_idx = gh[\"_global_index\"]\n",
    "        candidates_linkedin = set()\n",
    "        candidates_twitter = set()\n",
    "\n",
    "        # --- LinkedIn candidates ---\n",
    "        if gh[\"email_norm\"]:\n",
    "            for p in email_to_profiles[gh[\"email_norm\"]]:\n",
    "                if p[\"platform\"] == \"linkedin\":\n",
    "                    candidates_linkedin.add(p[\"_global_index\"])\n",
    "        if gh[\"first_name\"]:\n",
    "            for p in first_name_to_profiles[gh[\"first_name\"]]:\n",
    "                if p[\"platform\"] == \"linkedin\":\n",
    "                    candidates_linkedin.add(p[\"_global_index\"])\n",
    "        gh_text = (gh.get(\"bio\", \"\") + \" \" + gh.get(\"repo_descriptions\", \"\")).lower()\n",
    "        for p in linkedin_profiles:\n",
    "            if p.get(\"username\", \"\").lower() in gh_text:\n",
    "                candidates_linkedin.add(p[\"_global_index\"])\n",
    "\n",
    "        # --- Twitter candidates ---\n",
    "        if gh[\"email_norm\"]:\n",
    "            for p in email_to_profiles[gh[\"email_norm\"]]:\n",
    "                if p[\"platform\"] == \"twitter\":\n",
    "                    candidates_twitter.add(p[\"_global_index\"])\n",
    "        if gh[\"first_name\"]:\n",
    "            for p in first_name_to_profiles[gh[\"first_name\"]]:\n",
    "                if p[\"platform\"] == \"twitter\":\n",
    "                    candidates_twitter.add(p[\"_global_index\"])\n",
    "        for p in twitter_profiles:\n",
    "            if p.get(\"username\", \"\").lower() in gh_text:\n",
    "                candidates_twitter.add(p[\"_global_index\"])\n",
    "\n",
    "        # --- √âvaluer tous les triplets candidats ---\n",
    "        for li_idx in candidates_linkedin:\n",
    "            for tw_idx in candidates_twitter:\n",
    "                if len({gh_idx, li_idx, tw_idx}) < 3:\n",
    "                    continue\n",
    "\n",
    "                li = profile_to_index[li_idx]\n",
    "                tw = profile_to_index[tw_idx]\n",
    "\n",
    "                score = 0.0\n",
    "\n",
    "                # Email exact (0.30)\n",
    "                emails = {p[\"email_norm\"] for p in [gh, li, tw] if p[\"email_norm\"]}\n",
    "                if len(emails) == 1:\n",
    "                    score += 0.30\n",
    "\n",
    "                # Full name (0.25)\n",
    "                name_scores = [\n",
    "                    cosine_sim(embeddings[\"fullName\"][gh_idx], embeddings[\"fullName\"][li_idx]),\n",
    "                    cosine_sim(embeddings[\"fullName\"][gh_idx], embeddings[\"fullName\"][tw_idx]),\n",
    "                    cosine_sim(embeddings[\"fullName\"][li_idx], embeddings[\"fullName\"][tw_idx])\n",
    "                ]\n",
    "                score += 0.25 * max(name_scores)\n",
    "\n",
    "                # GitHub repo ‚Üî LinkedIn headline (0.30)\n",
    "                score += 0.30 * cosine_sim(\n",
    "                    embeddings[\"repo_descriptions\"][gh_idx],\n",
    "                    embeddings[\"headline\"][li_idx]\n",
    "                )\n",
    "\n",
    "                # Username (0.10)\n",
    "                user_scores = [\n",
    "                    cosine_sim(embeddings[\"username\"][gh_idx], embeddings[\"username\"][li_idx]),\n",
    "                    cosine_sim(embeddings[\"username\"][gh_idx], embeddings[\"username\"][tw_idx]),\n",
    "                    cosine_sim(embeddings[\"username\"][li_idx], embeddings[\"username\"][tw_idx])\n",
    "                ]\n",
    "                score += 0.10 * max(user_scores)\n",
    "\n",
    "                # Bio (0.05)\n",
    "                bio_vecs = []\n",
    "                for idx in [gh_idx, li_idx, tw_idx]:\n",
    "                    vec = embeddings[\"bio\"][idx]\n",
    "                    if np.linalg.norm(vec) > 1e-8:\n",
    "                        bio_vecs.append(vec)\n",
    "                if len(bio_vecs) >= 2:\n",
    "                    total = 0.0\n",
    "                    count = 0\n",
    "                    for i in range(len(bio_vecs)):\n",
    "                        for j in range(i + 1, len(bio_vecs)):\n",
    "                            total += cosine_sim(bio_vecs[i], bio_vecs[j])\n",
    "                            count += 1\n",
    "                    score += 0.05 * (total / count)\n",
    "\n",
    "                score = min(1.0, score)\n",
    "\n",
    "                if score >= 0.60:\n",
    "                    triplet_candidates.append((gh_idx, li_idx, tw_idx, score))\n",
    "\n",
    "    # Trier et s√©lectionner sans chevauchement\n",
    "    triplet_candidates.sort(key=lambda x: x[3], reverse=True)\n",
    "    used = set()\n",
    "    final_triplets = []\n",
    "\n",
    "    for gh_idx, li_idx, tw_idx, score in triplet_candidates:\n",
    "        ids = {gh_idx, li_idx, tw_idx}\n",
    "        if ids & used:\n",
    "            continue\n",
    "        used |= ids\n",
    "        final_triplets.append({\n",
    "            \"unified_id\": f\"person_{len(final_triplets):05d}\",\n",
    "            \"score\": score,  # sera converti en float natif\n",
    "            \"profiles\": [\n",
    "                profile_to_index[gh_idx],\n",
    "                profile_to_index[li_idx],\n",
    "                profile_to_index[tw_idx]\n",
    "            ]\n",
    "        })\n",
    "\n",
    "    # Sauvegarde s√©curis√©e (conversion NumPy ‚Üí JSON)\n",
    "    output_path = output_dir / \"unified_triplets.json\"\n",
    "    final_triplets_clean = convert_numpy_types(final_triplets)\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(final_triplets_clean, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    # Stats\n",
    "    print(f\"‚úÖ {len(final_triplets)} triplets complets identifi√©s\")\n",
    "    print(f\"‚úÖ R√©sultats sauvegard√©s dans '{output_path}'\")\n",
    "    print(\"‚û°Ô∏è Pr√™t pour analyse fine (ex: localisation Marocaine)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2605992a-39ae-4fd2-baef-90cb9000d91f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
