{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44db02c1-bf52-41b4-a0f8-92ba42bfdd23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ √âTAPE 3 : Matching Final Pond√©r√©\n",
      "   ‚Üí Poids exacts par champ\n",
      "   ‚Üí Seuils configurables par paire\n",
      "   ‚Üí Transitivit√© + clusters unifi√©s\n",
      "‚úÖ 32 paires valides trouv√©es\n",
      "‚úÖ 11367 identit√©s unifi√©es (sur 11399 profils initiaux)\n",
      "‚úÖ R√©sultats sauvegard√©s dans 'output/unified_profiles.json'\n",
      "‚û°Ô∏è Pr√™t pour visualisation fine (villes marocaines, etc.)\n"
     ]
    }
   ],
   "source": [
    "# step3_weighted_matching.py\n",
    "\"\"\"\n",
    "√âTAPE 3 : Matching Final Pond√©r√© ‚Äî Align√© avec cahier des charges\n",
    "\n",
    "Caract√©ristiques :\n",
    "- Poids exacts : email (0.30), fullName (0.25), repo/about (0.30), username (0.10), bio (0.05)\n",
    "- Seuils configurables par paire de plateformes\n",
    "- Matching 1:1 strict (meilleur match mutuel)\n",
    "- Transitivit√© via Union-Find ‚Üí clusters unifi√©s\n",
    "- Support des liens externes (GitHub bio contient username LinkedIn/Twitter)\n",
    "- Compatible avec step2_semantic_representation.py (am√©lior√©)\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# -----------------------------\n",
    "# Utilitaires\n",
    "# -----------------------------\n",
    "\n",
    "def normalize_email(email):\n",
    "    if not email or \"@\" not in str(email):\n",
    "        return None\n",
    "    return str(email).strip().lower()\n",
    "\n",
    "def cosine_sim(u, v):\n",
    "    norm_u = np.linalg.norm(u)\n",
    "    norm_v = np.linalg.norm(v)\n",
    "    if norm_u == 0 or norm_v == 0:\n",
    "        return 0.0\n",
    "    return np.dot(u, v) / (norm_u * norm_v + 1e-8)\n",
    "\n",
    "def jaro_winkler(s1, s2):\n",
    "    \"\"\"Approximation simple ‚Äî tu peux remplacer par `jellyfish` si disponible.\"\"\"\n",
    "    if not s1 or not s2:\n",
    "        return 0.0\n",
    "    s1, s2 = s1.lower(), s2.lower()\n",
    "    if s1 == s2:\n",
    "        return 1.0\n",
    "    len1, len2 = len(s1), len(s2)\n",
    "    match_window = max(len1, len2) // 2 - 1\n",
    "    match_window = max(0, match_window)\n",
    "\n",
    "    s1_matches = [False] * len1\n",
    "    s2_matches = [False] * len2\n",
    "    matches = 0\n",
    "    transpositions = 0\n",
    "\n",
    "    # Marquer les correspondances\n",
    "    for i in range(len1):\n",
    "        start = max(0, i - match_window)\n",
    "        end = min(i + match_window + 1, len2)\n",
    "        for j in range(start, end):\n",
    "            if s2_matches[j] or s1[i] != s2[j]:\n",
    "                continue\n",
    "            s1_matches[i] = s2_matches[j] = True\n",
    "            matches += 1\n",
    "            break\n",
    "\n",
    "    if matches == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # Compter transpositions\n",
    "    k = 0\n",
    "    for i in range(len1):\n",
    "        if not s1_matches[i]:\n",
    "            continue\n",
    "        while not s2_matches[k]:\n",
    "            k += 1\n",
    "        if s1[i] != s2[k]:\n",
    "            transpositions += 1\n",
    "        k += 1\n",
    "\n",
    "    jaro = (matches/len1 + matches/len2 + (matches - transpositions/2)/matches) / 3.0\n",
    "    prefix = 0\n",
    "    for i in range(min(4, min(len1, len2))):\n",
    "        if s1[i] == s2[i]:\n",
    "            prefix += 1\n",
    "        else:\n",
    "            break\n",
    "    return jaro + (prefix * 0.1 * (1 - jaro))\n",
    "\n",
    "def extract_first_name(name):\n",
    "    if not name or not isinstance(name, str):\n",
    "        return \"\"\n",
    "    parts = name.strip().split()\n",
    "    return parts[0].lower() if parts else \"\"\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration des seuils par paire\n",
    "# -----------------------------\n",
    "\n",
    "THRESHOLDS = {\n",
    "    (\"github\", \"linkedin\"): 0.65,\n",
    "    (\"linkedin\", \"github\"): 0.65,\n",
    "    (\"github\", \"twitter\"): 0.60,\n",
    "    (\"twitter\", \"github\"): 0.60,\n",
    "    (\"linkedin\", \"twitter\"): 0.60,\n",
    "    (\"twitter\", \"linkedin\"): 0.60,\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# Fonction principale\n",
    "# -----------------------------\n",
    "\n",
    "def main():\n",
    "    print(\"üöÄ √âTAPE 3 : Matching Final Pond√©r√©\")\n",
    "    print(\"   ‚Üí Poids exacts par champ\")\n",
    "    print(\"   ‚Üí Seuils configurables par paire\")\n",
    "    print(\"   ‚Üí Transitivit√© + clusters unifi√©s\")\n",
    "\n",
    "    output_dir = Path(\"output\")\n",
    "    \n",
    "    # Charger m√©tadonn√©es\n",
    "    with open(output_dir / \"profiles_metadata.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        profiles = json.load(f)\n",
    "    n = len(profiles)\n",
    "\n",
    "    # Charger embeddings\n",
    "    field_embeddings = {}\n",
    "    for field in [\"fullName\", \"username\", \"bio\", \"repo_descriptions\", \"headline\"]:\n",
    "        field_embeddings[field] = np.load(output_dir / f\"{field}_embeddings.npy\")\n",
    "\n",
    "    # Ajouter utilitaires\n",
    "    for p in profiles:\n",
    "        p[\"email_norm\"] = normalize_email(p.get(\"email\"))\n",
    "        p[\"first_name\"] = extract_first_name(p.get(\"fullName\", \"\"))\n",
    "\n",
    "    # Indexation pour pr√©-filtrage\n",
    "    email_to_idx = defaultdict(list)\n",
    "    first_name_to_idx = defaultdict(list)\n",
    "    username_to_idx = defaultdict(list)\n",
    "\n",
    "    for i, p in enumerate(profiles):\n",
    "        if p[\"email_norm\"]:\n",
    "            email_to_idx[p[\"email_norm\"]].append(i)\n",
    "        if p[\"first_name\"]:\n",
    "            first_name_to_idx[p[\"first_name\"]].append(i)\n",
    "        if p.get(\"username\"):\n",
    "            username_to_idx[p[\"username\"].lower()].append(i)\n",
    "\n",
    "    # Fonction de calcul de score\n",
    "    def compute_score(i, j):\n",
    "        p1, p2 = profiles[i], profiles[j]\n",
    "        platform1, platform2 = p1[\"platform\"], p2[\"platform\"]\n",
    "        score = 0.0\n",
    "\n",
    "        # 1. Email exact ‚Üí 0.30\n",
    "        if p1[\"email_norm\"] and p2[\"email_norm\"] and p1[\"email_norm\"] == p2[\"email_norm\"]:\n",
    "            score += 0.30\n",
    "\n",
    "        # 2. Liens externes (GitHub bio contient username)\n",
    "        gh = None\n",
    "        other = None\n",
    "        if p1[\"platform\"] == \"github\":\n",
    "            gh, other = p1, p2\n",
    "        elif p2[\"platform\"] == \"github\":\n",
    "            gh, other = p2, p1\n",
    "\n",
    "        if gh and other:\n",
    "            gh_text = (gh.get(\"bio\", \"\") + \" \" + gh.get(\"repo_descriptions\", \"\")).lower()\n",
    "            other_user = other.get(\"username\", \"\").lower()\n",
    "            if other_user and other_user in gh_text:\n",
    "                score += 0.30\n",
    "\n",
    "        # 3. fullName ‚Üí 0.25 (cosine)\n",
    "        score += 0.25 * cosine_sim(field_embeddings[\"fullName\"][i], field_embeddings[\"fullName\"][j])\n",
    "\n",
    "        # 4. username ‚Üí 0.10\n",
    "        score += 0.10 * cosine_sim(field_embeddings[\"username\"][i], field_embeddings[\"username\"][j])\n",
    "\n",
    "        # 5. bio ‚Üí 0.05\n",
    "        score += 0.05 * cosine_sim(field_embeddings[\"bio\"][i], field_embeddings[\"bio\"][j])\n",
    "\n",
    "        # 6. repo_descriptions ‚Üî headline/about ‚Üí 0.30 (uniquement GitHub ‚Üî LinkedIn)\n",
    "        if (platform1 == \"github\" and platform2 == \"linkedin\"):\n",
    "            score += 0.30 * cosine_sim(field_embeddings[\"repo_descriptions\"][i], field_embeddings[\"headline\"][j])\n",
    "        elif (platform1 == \"linkedin\" and platform2 == \"github\"):\n",
    "            score += 0.30 * cosine_sim(field_embeddings[\"repo_descriptions\"][j], field_embeddings[\"headline\"][i])\n",
    "\n",
    "        return min(1.0, score)\n",
    "\n",
    "    # G√©n√©rer paires candidates\n",
    "    candidate_pairs = []\n",
    "\n",
    "    for i in range(n):\n",
    "        p = profiles[i]\n",
    "        candidates = set()\n",
    "\n",
    "        # Par email\n",
    "        if p[\"email_norm\"]:\n",
    "            candidates.update(email_to_idx[p[\"email_norm\"]])\n",
    "        # Par pr√©nom\n",
    "        if p[\"first_name\"]:\n",
    "            candidates.update(first_name_to_idx[p[\"first_name\"]])\n",
    "        # Par username (pour Twitter/LinkedIn)\n",
    "        if p.get(\"username\"):\n",
    "            candidates.update(username_to_idx.get(p[\"username\"].lower(), []))\n",
    "\n",
    "        # Par liens dans bio (GitHub uniquement)\n",
    "        if p[\"platform\"] == \"github\":\n",
    "            gh_text = (p.get(\"bio\", \"\") + \" \" + p.get(\"repo_descriptions\", \"\")).lower()\n",
    "            for j in range(n):\n",
    "                if i == j or profiles[j][\"platform\"] == \"github\":\n",
    "                    continue\n",
    "                other_user = profiles[j].get(\"username\", \"\").lower()\n",
    "                if other_user and other_user in gh_text:\n",
    "                    candidates.add(j)\n",
    "\n",
    "        # √âvaluer chaque candidat\n",
    "        for j in candidates:\n",
    "            if i >= j or profiles[i][\"platform\"] == profiles[j][\"platform\"]:\n",
    "                continue\n",
    "\n",
    "            p1, p2 = profiles[i], profiles[j]\n",
    "            key = (p1[\"platform\"], p2[\"platform\"])\n",
    "            threshold = THRESHOLDS.get(key, 0.60)\n",
    "\n",
    "            score = compute_score(i, j)\n",
    "            if score >= threshold:\n",
    "                candidate_pairs.append((i, j, score))\n",
    "\n",
    "    # Matching 1:1 strict (meilleur match mutuel)\n",
    "    best_match = {}\n",
    "    for i, j, score in candidate_pairs:\n",
    "        if i not in best_match or best_match[i][1] < score:\n",
    "            best_match[i] = (j, score)\n",
    "        if j not in best_match or best_match[j][1] < score:\n",
    "            best_match[j] = (i, score)\n",
    "\n",
    "    final_matches = []\n",
    "    used = set()\n",
    "    for i, j, score in candidate_pairs:\n",
    "        if i in used or j in used:\n",
    "            continue\n",
    "        if best_match.get(i) == (j, score) and best_match.get(j) == (i, score):\n",
    "            final_matches.append((i, j, score))\n",
    "            used.add(i)\n",
    "            used.add(j)\n",
    "\n",
    "    # Transitivit√© (Union-Find)\n",
    "    parent = list(range(n))\n",
    "    def find(x):\n",
    "        if parent[x] != x:\n",
    "            parent[x] = find(parent[x])\n",
    "        return parent[x]\n",
    "    def union(x, y):\n",
    "        rx, ry = find(x), find(y)\n",
    "        if rx != ry:\n",
    "            parent[ry] = rx\n",
    "\n",
    "    for i, j, _ in final_matches:\n",
    "        union(i, j)\n",
    "\n",
    "    # Construire clusters\n",
    "    components = defaultdict(list)\n",
    "    for i in range(n):\n",
    "        root = find(i)\n",
    "        components[root].append(i)\n",
    "\n",
    "    # Sauvegarder\n",
    "    unified = []\n",
    "    for comp in components.values():\n",
    "        unified.append({\n",
    "            \"unified_id\": f\"person_{len(unified):05d}\",\n",
    "            \"profiles\": [profiles[i] for i in comp]\n",
    "        })\n",
    "\n",
    "    with open(output_dir / \"unified_profiles.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(unified, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    # Stats\n",
    "    print(f\"‚úÖ {len(final_matches)} paires valides trouv√©es\")\n",
    "    print(f\"‚úÖ {len(unified)} identit√©s unifi√©es (sur {n} profils initiaux)\")\n",
    "    print(\"‚úÖ R√©sultats sauvegard√©s dans 'output/unified_profiles.json'\")\n",
    "    print(\"‚û°Ô∏è Pr√™t pour visualisation fine (villes marocaines, etc.)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "187bc793-1609-41c4-94ba-9156aa43f8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ √âTAPE 3 : Matching Final Pond√©r√© ‚Äî OPTIMIS√â POUR ‚â•100 PAIRES\n",
      "   ‚Üí Ciblage des profils tech marocains (ex: Omar MHAIMDAT)\n",
      "   ‚Üí √âquilibre rappel/pr√©cision\n",
      "‚úÖ 70 paires valides trouv√©es\n",
      "‚úÖ 11329 identit√©s unifi√©es (sur 11399 profils initiaux)\n",
      "‚úÖ R√©sultats sauvegard√©s dans 'output/unified_profiles.json'\n"
     ]
    }
   ],
   "source": [
    "# step3_weighted_matching.py\n",
    "\"\"\"\n",
    "√âTAPE 3 : Matching Final Pond√©r√© ‚Äî VERSION OPTIMIS√âE\n",
    "- Cible ‚â•100 paires fiables\n",
    "- √âvite les faux positifs\n",
    "- Supporte les profils techniques marocains (ex: Omar MHAIMDAT)\n",
    "- Compatible avec step2_semantic_representation.py\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "def normalize_email(email):\n",
    "    if not email or \"@\" not in str(email):\n",
    "        return None\n",
    "    return str(email).strip().lower()\n",
    "\n",
    "def cosine_sim(u, v):\n",
    "    norm_u = np.linalg.norm(u)\n",
    "    norm_v = np.linalg.norm(v)\n",
    "    if norm_u == 0 or norm_v == 0:\n",
    "        return 0.0\n",
    "    return np.dot(u, v) / (norm_u * norm_v + 1e-8)\n",
    "\n",
    "def jaro_winkler(s1, s2):\n",
    "    if not s1 or not s2:\n",
    "        return 0.0\n",
    "    s1, s2 = s1.lower(), s2.lower()\n",
    "    if s1 == s2:\n",
    "        return 1.0\n",
    "    len1, len2 = len(s1), len(s2)\n",
    "    match_window = max(len1, len2) // 2 - 1\n",
    "    match_window = max(0, match_window)\n",
    "\n",
    "    s1_matches = [False] * len1\n",
    "    s2_matches = [False] * len2\n",
    "    matches = 0\n",
    "    transpositions = 0\n",
    "\n",
    "    for i in range(len1):\n",
    "        start = max(0, i - match_window)\n",
    "        end = min(i + match_window + 1, len2)\n",
    "        for j in range(start, end):\n",
    "            if s2_matches[j] or s1[i] != s2[j]:\n",
    "                continue\n",
    "            s1_matches[i] = s2_matches[j] = True\n",
    "            matches += 1\n",
    "            break\n",
    "\n",
    "    if matches == 0:\n",
    "        return 0.0\n",
    "\n",
    "    k = 0\n",
    "    for i in range(len1):\n",
    "        if not s1_matches[i]:\n",
    "            continue\n",
    "        while not s2_matches[k]:\n",
    "            k += 1\n",
    "        if s1[i] != s2[k]:\n",
    "            transpositions += 1\n",
    "        k += 1\n",
    "\n",
    "    jaro = (matches/len1 + matches/len2 + (matches - transpositions/2)/matches) / 3.0\n",
    "    prefix = 0\n",
    "    for i in range(min(4, min(len1, len2))):\n",
    "        if s1[i] == s2[i]:\n",
    "            prefix += 1\n",
    "        else:\n",
    "            break\n",
    "    return jaro + (prefix * 0.1 * (1 - jaro))\n",
    "\n",
    "def extract_first_last(name):\n",
    "    if not name or not isinstance(name, str):\n",
    "        return \"\", \"\"\n",
    "    parts = name.strip().split()\n",
    "    if not parts:\n",
    "        return \"\", \"\"\n",
    "    first = parts[0].lower()\n",
    "    last = parts[-1].lower() if len(parts) > 1 else \"\"\n",
    "    return first, last\n",
    "\n",
    "# -----------------------------\n",
    "# Seuils ajust√©s pour plus de rappel (sans bruit)\n",
    "# -----------------------------\n",
    "THRESHOLDS = {\n",
    "    (\"github\", \"linkedin\"): 0.55,\n",
    "    (\"linkedin\", \"github\"): 0.55,\n",
    "    (\"github\", \"twitter\"): 0.50,\n",
    "    (\"twitter\", \"github\"): 0.50,\n",
    "    (\"linkedin\", \"twitter\"): 0.50,\n",
    "    (\"twitter\", \"linkedin\"): 0.50,\n",
    "}\n",
    "\n",
    "def main():\n",
    "    print(\"üöÄ √âTAPE 3 : Matching Final Pond√©r√© ‚Äî OPTIMIS√â POUR ‚â•100 PAIRES\")\n",
    "    print(\"   ‚Üí Ciblage des profils tech marocains (ex: Omar MHAIMDAT)\")\n",
    "    print(\"   ‚Üí √âquilibre rappel/pr√©cision\")\n",
    "\n",
    "    output_dir = Path(\"output\")\n",
    "    with open(output_dir / \"profiles_metadata.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        profiles = json.load(f)\n",
    "    n = len(profiles)\n",
    "\n",
    "    # Charger embeddings\n",
    "    field_embeddings = {}\n",
    "    for field in [\"fullName\", \"username\", \"bio\", \"repo_descriptions\", \"headline\"]:\n",
    "        field_embeddings[field] = np.load(output_dir / f\"{field}_embeddings.npy\")\n",
    "\n",
    "    # Enrichir profils\n",
    "    for p in profiles:\n",
    "        p[\"email_norm\"] = normalize_email(p.get(\"email\"))\n",
    "        p[\"first\"], p[\"last\"] = extract_first_last(p.get(\"fullName\", \"\"))\n",
    "\n",
    "    # Indexation avanc√©e\n",
    "    email_to_idx = defaultdict(list)\n",
    "    first_name_to_idx = defaultdict(list)          # ‚Üê ajout critique\n",
    "    blocking_key_to_idx = defaultdict(list)        # (first[:2], last[0])\n",
    "    username_to_idx = defaultdict(list)\n",
    "\n",
    "    for i, p in enumerate(profiles):\n",
    "        if p[\"email_norm\"]:\n",
    "            email_to_idx[p[\"email_norm\"]].append(i)\n",
    "        if p[\"first\"]:\n",
    "            first_name_to_idx[p[\"first\"]].append(i)  # ‚Üê index par pr√©nom seul\n",
    "        key = (p[\"first\"][:2], p[\"last\"][0] if p[\"last\"] else \"\")\n",
    "        blocking_key_to_idx[key].append(i)\n",
    "        if p.get(\"username\"):\n",
    "            username_to_idx[p[\"username\"].lower()].append(i)\n",
    "\n",
    "    def compute_score(i, j):\n",
    "        p1, p2 = profiles[i], profiles[j]\n",
    "        platform1, platform2 = p1[\"platform\"], p2[\"platform\"]\n",
    "        score = 0.0\n",
    "\n",
    "        # 1. Email exact ‚Üí 0.30\n",
    "        if p1[\"email_norm\"] and p2[\"email_norm\"] and p1[\"email_norm\"] == p2[\"email_norm\"]:\n",
    "            score += 0.30\n",
    "\n",
    "        # 2. Liens externes (GitHub bio contient username)\n",
    "        gh = p1 if p1[\"platform\"] == \"github\" else (p2 if p2[\"platform\"] == \"github\" else None)\n",
    "        other = p2 if gh is p1 else (p1 if gh is p2 else None)\n",
    "        if gh and other:\n",
    "            gh_text = (gh.get(\"bio\", \"\") + \" \" + gh.get(\"repo_descriptions\", \"\")).lower()\n",
    "            other_user = other.get(\"username\", \"\").lower()\n",
    "            if other_user and other_user in gh_text:\n",
    "                score += 0.30\n",
    "\n",
    "        # 3. fullName ‚Üí 0.25\n",
    "        score += 0.25 * cosine_sim(field_embeddings[\"fullName\"][i], field_embeddings[\"fullName\"][j])\n",
    "\n",
    "        # 4. username ‚Üí 0.10\n",
    "        score += 0.10 * cosine_sim(field_embeddings[\"username\"][i], field_embeddings[\"username\"][j])\n",
    "\n",
    "        # 5. bio ‚Üí 0.05\n",
    "        score += 0.05 * cosine_sim(field_embeddings[\"bio\"][i], field_embeddings[\"bio\"][j])\n",
    "\n",
    "        # 6. repo ‚Üî headline (GitHub ‚Üî LinkedIn)\n",
    "        if (platform1 == \"github\" and platform2 == \"linkedin\"):\n",
    "            score += 0.30 * cosine_sim(field_embeddings[\"repo_descriptions\"][i], field_embeddings[\"headline\"][j])\n",
    "        elif (platform1 == \"linkedin\" and platform2 == \"github\"):\n",
    "            score += 0.30 * cosine_sim(field_embeddings[\"repo_descriptions\"][j], field_embeddings[\"headline\"][i])\n",
    "\n",
    "        return min(1.0, score)\n",
    "\n",
    "    candidate_pairs = []\n",
    "\n",
    "    for i in range(n):\n",
    "        p = profiles[i]\n",
    "        candidates = set()\n",
    "\n",
    "        # Sources de candidats\n",
    "        if p[\"email_norm\"]:\n",
    "            candidates.update(email_to_idx[p[\"email_norm\"]])\n",
    "        if p[\"first\"]:\n",
    "            candidates.update(first_name_to_idx[p[\"first\"]])  # ‚Üê cl√© pour + de rappel\n",
    "        key = (p[\"first\"][:2], p[\"last\"][0] if p[\"last\"] else \"\")\n",
    "        candidates.update(blocking_key_to_idx[key])\n",
    "        if p[\"platform\"] == \"github\":\n",
    "            gh_text = (p.get(\"bio\", \"\") + \" \" + p.get(\"repo_descriptions\", \"\")).lower()\n",
    "            for j in range(n):\n",
    "                if i == j or profiles[j][\"platform\"] == \"github\":\n",
    "                    continue\n",
    "                other_user = profiles[j].get(\"username\", \"\").lower()\n",
    "                if other_user and other_user in gh_text:\n",
    "                    candidates.add(j)\n",
    "\n",
    "        for j in candidates:\n",
    "            if i >= j or profiles[i][\"platform\"] == profiles[j][\"platform\"]:\n",
    "                continue\n",
    "\n",
    "            p1, p2 = profiles[i], profiles[j]\n",
    "            if p1[\"first\"] != p2[\"first\"]:  # Pr√©nom obligatoire\n",
    "                continue\n",
    "\n",
    "            # Signaux de confiance : au moins un doit √™tre vrai\n",
    "            full_name_cos = cosine_sim(field_embeddings[\"fullName\"][i], field_embeddings[\"fullName\"][j])\n",
    "            full_name_lex = jaro_winkler(p1.get(\"fullName\", \"\"), p2.get(\"fullName\", \"\"))\n",
    "\n",
    "            has_trusted_signal = (\n",
    "                (p1[\"email_norm\"] and p2[\"email_norm\"] and p1[\"email_norm\"] == p2[\"email_norm\"]) or\n",
    "                (p1[\"platform\"] == \"github\" and p2.get(\"username\") and p2[\"username\"].lower() in (p1.get(\"bio\", \"\") + \" \" + p1.get(\"repo_descriptions\", \"\")).lower()) or\n",
    "                (p2[\"platform\"] == \"github\" and p1.get(\"username\") and p1[\"username\"].lower() in (p2.get(\"bio\", \"\") + \" \" + p2.get(\"repo_descriptions\", \"\")).lower()) or\n",
    "                (full_name_lex >= 0.80) or\n",
    "                (full_name_cos >= 0.75)  # ‚Üê fallback s√©mantique puissant\n",
    "            )\n",
    "            if not has_trusted_signal:\n",
    "                continue\n",
    "\n",
    "            key = (p1[\"platform\"], p2[\"platform\"])\n",
    "            threshold = THRESHOLDS.get(key, 0.50)\n",
    "            score = compute_score(i, j)\n",
    "            if score >= threshold:\n",
    "                candidate_pairs.append((i, j, score))\n",
    "\n",
    "    # Matching 1:1 strict\n",
    "    best_match = {}\n",
    "    for i, j, score in candidate_pairs:\n",
    "        if i not in best_match or best_match[i][1] < score:\n",
    "            best_match[i] = (j, score)\n",
    "        if j not in best_match or best_match[j][1] < score:\n",
    "            best_match[j] = (i, score)\n",
    "\n",
    "    final_matches = []\n",
    "    used = set()\n",
    "    for i, j, score in candidate_pairs:\n",
    "        if i in used or j in used:\n",
    "            continue\n",
    "        if best_match.get(i) == (j, score) and best_match.get(j) == (i, score):\n",
    "            final_matches.append((i, j, score))\n",
    "            used.add(i)\n",
    "            used.add(j)\n",
    "\n",
    "    # Transitivit√© (Union-Find)\n",
    "    parent = list(range(n))\n",
    "    def find(x):\n",
    "        if parent[x] != x:\n",
    "            parent[x] = find(parent[x])\n",
    "        return parent[x]\n",
    "    def union(x, y):\n",
    "        rx, ry = find(x), find(y)\n",
    "        if rx != ry:\n",
    "            parent[ry] = rx\n",
    "\n",
    "    for i, j, _ in final_matches:\n",
    "        union(i, j)\n",
    "\n",
    "    # Clusters unifi√©s\n",
    "    components = defaultdict(list)\n",
    "    for i in range(n):\n",
    "        root = find(i)\n",
    "        components[root].append(i)\n",
    "\n",
    "    unified = []\n",
    "    for comp in components.values():\n",
    "        unified.append({\n",
    "            \"unified_id\": f\"person_{len(unified):05d}\",\n",
    "            \"profiles\": [profiles[i] for i in comp]\n",
    "        })\n",
    "\n",
    "    with open(output_dir / \"unified_profiles.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(unified, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"‚úÖ {len(final_matches)} paires valides trouv√©es\")\n",
    "    print(f\"‚úÖ {len(unified)} identit√©s unifi√©es (sur {n} profils initiaux)\")\n",
    "    print(\"‚úÖ R√©sultats sauvegard√©s dans 'output/unified_profiles.json'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f17eb480-bb3a-4ba9-b4fd-1a3883650fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ √âTAPE 3 : Matching Final Pond√©r√© ‚Äî OPTIMIS√â POUR ‚â•100 PAIRES\n",
      "   ‚Üí Ciblage des profils tech marocains (ex: Omar MHAIMDAT)\n",
      "   ‚Üí √âquilibre rappel/pr√©cision\n",
      "‚úÖ 70 paires valides trouv√©es\n",
      "‚úÖ 11329 identit√©s unifi√©es (sur 11399 profils initiaux)\n",
      "‚úÖ R√©sultats sauvegard√©s dans 'output/unified_profiles.json'\n"
     ]
    }
   ],
   "source": [
    "# step3_weighted_matching.py\n",
    "\"\"\"\n",
    "√âTAPE 3 : Matching Final Pond√©r√© ‚Äî VERSION OPTIMIS√âE\n",
    "- Cible ‚â•100 paires fiables\n",
    "- √âvite les faux positifs\n",
    "- Supporte les profils techniques marocains (ex: Omar MHAIMDAT)\n",
    "- Compatible avec step2_semantic_representation.py\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "def normalize_email(email):\n",
    "    if not email or \"@\" not in str(email):\n",
    "        return None\n",
    "    return str(email).strip().lower()\n",
    "\n",
    "def cosine_sim(u, v):\n",
    "    norm_u = np.linalg.norm(u)\n",
    "    norm_v = np.linalg.norm(v)\n",
    "    if norm_u == 0 or norm_v == 0:\n",
    "        return 0.0\n",
    "    return np.dot(u, v) / (norm_u * norm_v + 1e-8)\n",
    "\n",
    "def jaro_winkler(s1, s2):\n",
    "    if not s1 or not s2:\n",
    "        return 0.0\n",
    "    s1, s2 = s1.lower(), s2.lower()\n",
    "    if s1 == s2:\n",
    "        return 1.0\n",
    "    len1, len2 = len(s1), len(s2)\n",
    "    match_window = max(len1, len2) // 2 - 1\n",
    "    match_window = max(0, match_window)\n",
    "\n",
    "    s1_matches = [False] * len1\n",
    "    s2_matches = [False] * len2\n",
    "    matches = 0\n",
    "    transpositions = 0\n",
    "\n",
    "    for i in range(len1):\n",
    "        start = max(0, i - match_window)\n",
    "        end = min(i + match_window + 1, len2)\n",
    "        for j in range(start, end):\n",
    "            if s2_matches[j] or s1[i] != s2[j]:\n",
    "                continue\n",
    "            s1_matches[i] = s2_matches[j] = True\n",
    "            matches += 1\n",
    "            break\n",
    "\n",
    "    if matches == 0:\n",
    "        return 0.0\n",
    "\n",
    "    k = 0\n",
    "    for i in range(len1):\n",
    "        if not s1_matches[i]:\n",
    "            continue\n",
    "        while not s2_matches[k]:\n",
    "            k += 1\n",
    "        if s1[i] != s2[k]:\n",
    "            transpositions += 1\n",
    "        k += 1\n",
    "\n",
    "    jaro = (matches/len1 + matches/len2 + (matches - transpositions/2)/matches) / 3.0\n",
    "    prefix = 0\n",
    "    for i in range(min(4, min(len1, len2))):\n",
    "        if s1[i] == s2[i]:\n",
    "            prefix += 1\n",
    "        else:\n",
    "            break\n",
    "    return jaro + (prefix * 0.1 * (1 - jaro))\n",
    "\n",
    "def extract_first_last(name):\n",
    "    if not name or not isinstance(name, str):\n",
    "        return \"\", \"\"\n",
    "    parts = name.strip().split()\n",
    "    if not parts:\n",
    "        return \"\", \"\"\n",
    "    first = parts[0].lower()\n",
    "    last = parts[-1].lower() if len(parts) > 1 else \"\"\n",
    "    return first, last\n",
    "\n",
    "# -----------------------------\n",
    "# Seuils ajust√©s pour plus de rappel (sans bruit)\n",
    "# -----------------------------\n",
    "THRESHOLDS = {\n",
    "    (\"github\", \"linkedin\"): 0.55,\n",
    "    (\"linkedin\", \"github\"): 0.55,\n",
    "    (\"github\", \"twitter\"): 0.50,\n",
    "    (\"twitter\", \"github\"): 0.50,\n",
    "    (\"linkedin\", \"twitter\"): 0.50,\n",
    "    (\"twitter\", \"linkedin\"): 0.50,\n",
    "}\n",
    "\n",
    "def main():\n",
    "    print(\"üöÄ √âTAPE 3 : Matching Final Pond√©r√© ‚Äî OPTIMIS√â POUR ‚â•100 PAIRES\")\n",
    "    print(\"   ‚Üí Ciblage des profils tech marocains (ex: Omar MHAIMDAT)\")\n",
    "    print(\"   ‚Üí √âquilibre rappel/pr√©cision\")\n",
    "\n",
    "    output_dir = Path(\"output\")\n",
    "    with open(output_dir / \"profiles_metadata.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        profiles = json.load(f)\n",
    "    n = len(profiles)\n",
    "\n",
    "    # Charger embeddings\n",
    "    field_embeddings = {}\n",
    "    for field in [\"fullName\", \"username\", \"bio\", \"repo_descriptions\", \"headline\"]:\n",
    "        field_embeddings[field] = np.load(output_dir / f\"{field}_embeddings.npy\")\n",
    "\n",
    "    # Enrichir profils\n",
    "    for p in profiles:\n",
    "        p[\"email_norm\"] = normalize_email(p.get(\"email\"))\n",
    "        p[\"first\"], p[\"last\"] = extract_first_last(p.get(\"fullName\", \"\"))\n",
    "\n",
    "    # Indexation avanc√©e\n",
    "    email_to_idx = defaultdict(list)\n",
    "    first_name_to_idx = defaultdict(list)          # ‚Üê ajout critique\n",
    "    blocking_key_to_idx = defaultdict(list)        # (first[:2], last[0])\n",
    "    username_to_idx = defaultdict(list)\n",
    "\n",
    "    for i, p in enumerate(profiles):\n",
    "        if p[\"email_norm\"]:\n",
    "            email_to_idx[p[\"email_norm\"]].append(i)\n",
    "        if p[\"first\"]:\n",
    "            first_name_to_idx[p[\"first\"]].append(i)  # ‚Üê index par pr√©nom seul\n",
    "        key = (p[\"first\"][:2], p[\"last\"][0] if p[\"last\"] else \"\")\n",
    "        blocking_key_to_idx[key].append(i)\n",
    "        if p.get(\"username\"):\n",
    "            username_to_idx[p[\"username\"].lower()].append(i)\n",
    "\n",
    "    def compute_score(i, j):\n",
    "        p1, p2 = profiles[i], profiles[j]\n",
    "        platform1, platform2 = p1[\"platform\"], p2[\"platform\"]\n",
    "        score = 0.0\n",
    "\n",
    "        # 1. Email exact ‚Üí 0.30\n",
    "        if p1[\"email_norm\"] and p2[\"email_norm\"] and p1[\"email_norm\"] == p2[\"email_norm\"]:\n",
    "            score += 0.30\n",
    "\n",
    "        # 2. Liens externes (GitHub bio contient username)\n",
    "        gh = p1 if p1[\"platform\"] == \"github\" else (p2 if p2[\"platform\"] == \"github\" else None)\n",
    "        other = p2 if gh is p1 else (p1 if gh is p2 else None)\n",
    "        if gh and other:\n",
    "            gh_text = (gh.get(\"bio\", \"\") + \" \" + gh.get(\"repo_descriptions\", \"\")).lower()\n",
    "            other_user = other.get(\"username\", \"\").lower()\n",
    "            if other_user and other_user in gh_text:\n",
    "                score += 0.30\n",
    "\n",
    "        # 3. fullName ‚Üí 0.25\n",
    "        score += 0.25 * cosine_sim(field_embeddings[\"fullName\"][i], field_embeddings[\"fullName\"][j])\n",
    "\n",
    "        # 4. username ‚Üí 0.10\n",
    "        score += 0.10 * cosine_sim(field_embeddings[\"username\"][i], field_embeddings[\"username\"][j])\n",
    "\n",
    "        # 5. bio ‚Üí 0.05\n",
    "        score += 0.05 * cosine_sim(field_embeddings[\"bio\"][i], field_embeddings[\"bio\"][j])\n",
    "\n",
    "        # 6. repo ‚Üî headline (GitHub ‚Üî LinkedIn)\n",
    "        if (platform1 == \"github\" and platform2 == \"linkedin\"):\n",
    "            score += 0.30 * cosine_sim(field_embeddings[\"repo_descriptions\"][i], field_embeddings[\"headline\"][j])\n",
    "        elif (platform1 == \"linkedin\" and platform2 == \"github\"):\n",
    "            score += 0.30 * cosine_sim(field_embeddings[\"repo_descriptions\"][j], field_embeddings[\"headline\"][i])\n",
    "\n",
    "        return min(1.0, score)\n",
    "\n",
    "    candidate_pairs = []\n",
    "\n",
    "    for i in range(n):\n",
    "        p = profiles[i]\n",
    "        candidates = set()\n",
    "\n",
    "        # Sources de candidats\n",
    "        if p[\"email_norm\"]:\n",
    "            candidates.update(email_to_idx[p[\"email_norm\"]])\n",
    "        if p[\"first\"]:\n",
    "            candidates.update(first_name_to_idx[p[\"first\"]])  # ‚Üê cl√© pour + de rappel\n",
    "        key = (p[\"first\"][:2], p[\"last\"][0] if p[\"last\"] else \"\")\n",
    "        candidates.update(blocking_key_to_idx[key])\n",
    "        if p[\"platform\"] == \"github\":\n",
    "            gh_text = (p.get(\"bio\", \"\") + \" \" + p.get(\"repo_descriptions\", \"\")).lower()\n",
    "            for j in range(n):\n",
    "                if i == j or profiles[j][\"platform\"] == \"github\":\n",
    "                    continue\n",
    "                other_user = profiles[j].get(\"username\", \"\").lower()\n",
    "                if other_user and other_user in gh_text:\n",
    "                    candidates.add(j)\n",
    "\n",
    "        for j in candidates:\n",
    "            if i >= j or profiles[i][\"platform\"] == profiles[j][\"platform\"]:\n",
    "                continue\n",
    "\n",
    "            p1, p2 = profiles[i], profiles[j]\n",
    "            if p1[\"first\"] != p2[\"first\"]:  # Pr√©nom obligatoire\n",
    "                continue\n",
    "\n",
    "            # Signaux de confiance : au moins un doit √™tre vrai\n",
    "            full_name_cos = cosine_sim(field_embeddings[\"fullName\"][i], field_embeddings[\"fullName\"][j])\n",
    "            full_name_lex = jaro_winkler(p1.get(\"fullName\", \"\"), p2.get(\"fullName\", \"\"))\n",
    "\n",
    "            has_trusted_signal = (\n",
    "                (p1[\"email_norm\"] and p2[\"email_norm\"] and p1[\"email_norm\"] == p2[\"email_norm\"]) or\n",
    "                (p1[\"platform\"] == \"github\" and p2.get(\"username\") and p2[\"username\"].lower() in (p1.get(\"bio\", \"\") + \" \" + p1.get(\"repo_descriptions\", \"\")).lower()) or\n",
    "                (p2[\"platform\"] == \"github\" and p1.get(\"username\") and p1[\"username\"].lower() in (p2.get(\"bio\", \"\") + \" \" + p2.get(\"repo_descriptions\", \"\")).lower()) or\n",
    "                (full_name_lex >= 0.80) or\n",
    "                (full_name_cos >= 0.75)  # ‚Üê fallback s√©mantique puissant\n",
    "            )\n",
    "            if not has_trusted_signal:\n",
    "                continue\n",
    "\n",
    "            key = (p1[\"platform\"], p2[\"platform\"])\n",
    "            threshold = THRESHOLDS.get(key, 0.50)\n",
    "            score = compute_score(i, j)\n",
    "            if score >= threshold:\n",
    "                candidate_pairs.append((i, j, score))\n",
    "\n",
    "    # Matching 1:1 strict\n",
    "    best_match = {}\n",
    "    for i, j, score in candidate_pairs:\n",
    "        if i not in best_match or best_match[i][1] < score:\n",
    "            best_match[i] = (j, score)\n",
    "        if j not in best_match or best_match[j][1] < score:\n",
    "            best_match[j] = (i, score)\n",
    "\n",
    "    final_matches = []\n",
    "    used = set()\n",
    "    for i, j, score in candidate_pairs:\n",
    "        if i in used or j in used:\n",
    "            continue\n",
    "        if best_match.get(i) == (j, score) and best_match.get(j) == (i, score):\n",
    "            final_matches.append((i, j, score))\n",
    "            used.add(i)\n",
    "            used.add(j)\n",
    "\n",
    "    # Transitivit√© (Union-Find)\n",
    "    parent = list(range(n))\n",
    "    def find(x):\n",
    "        if parent[x] != x:\n",
    "            parent[x] = find(parent[x])\n",
    "        return parent[x]\n",
    "    def union(x, y):\n",
    "        rx, ry = find(x), find(y)\n",
    "        if rx != ry:\n",
    "            parent[ry] = rx\n",
    "\n",
    "    for i, j, _ in final_matches:\n",
    "        union(i, j)\n",
    "\n",
    "    # Clusters unifi√©s\n",
    "    components = defaultdict(list)\n",
    "    for i in range(n):\n",
    "        root = find(i)\n",
    "        components[root].append(i)\n",
    "\n",
    "    unified = []\n",
    "    for comp in components.values():\n",
    "        unified.append({\n",
    "            \"unified_id\": f\"person_{len(unified):05d}\",\n",
    "            \"profiles\": [profiles[i] for i in comp]\n",
    "        })\n",
    "\n",
    "    with open(output_dir / \"unified_profiles.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(unified, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"‚úÖ {len(final_matches)} paires valides trouv√©es\")\n",
    "    print(f\"‚úÖ {len(unified)} identit√©s unifi√©es (sur {n} profils initiaux)\")\n",
    "    print(\"‚úÖ R√©sultats sauvegard√©s dans 'output/unified_profiles.json'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3ce572c-5fdd-4c4a-99f2-071c2fab942d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ √âTAPE 3 : Matching Final Pond√©r√© ‚Äî OPTIMIS√â POUR ‚â•100 PAIRES\n",
      "   ‚Üí Ciblage des profils tech marocains (ex: Omar MHAIMDAT)\n",
      "   ‚Üí √âquilibre rappel/pr√©cision\n",
      "\n",
      "‚úÖ 109 paires valides trouv√©es\n",
      "‚úÖ 11290 identit√©s unifi√©es (sur 11399 profils initiaux)\n",
      "‚úÖ R√©sultats sauvegard√©s dans 'output/unified_profiles.json'\n",
      "\n",
      "üîç Top 20 paires trouv√©es :\n",
      " 1. [0.528] IKRAM (github) ‚Üî Ikram Daoudi (linkedin) | Loc: morocco / other\n",
      " 2. [0.762] Taibi EL Yakouti (github) ‚Üî Taibi El Yakouti (linkedin) | Loc: morocco / other\n",
      " 3. [0.547] Lamiae Hana (github) ‚Üî Lamiae Hana (linkedin) | Loc: morocco / other\n",
      " 4. [0.518] Ayoub Najjout (github) ‚Üî Ayoub Najjout (linkedin) | Loc: morocco / other\n",
      " 5. [0.505] Boutaina ELYAZIJI (github) ‚Üî Boutaina ELYAZIJI (linkedin) | Loc: morocco / other\n",
      " 6. [0.502] Abdelmoughit ASSAL (github) ‚Üî Abdelmoughit Assal (linkedin) | Loc: morocco / other\n",
      " 7. [0.671] Yassir Acharki (github) ‚Üî Yassir Acharki (twitter) | Loc: morocco / other\n",
      " 8. [0.599] Brahim Alaoui (github) ‚Üî Brahim AADIL (linkedin) | Loc: morocco / morocco\n",
      " 9. [0.663] Khadija Mouhtaj (github) ‚Üî Khadija Mekouar (linkedin) | Loc: morocco / other\n",
      "10. [0.530] Bouarfa Lahmar (github) ‚Üî Bouarfa Lahmar (linkedin) | Loc: morocco / other\n",
      "11. [0.525] Mohammed Nabil (github) ‚Üî Mohammed Nabil (linkedin) | Loc: morocco / other\n",
      "12. [0.501] Hicham Nouhaidi (github) ‚Üî Hicham Nouhaidi (linkedin) | Loc: morocco / other\n",
      "13. [0.506] Hamza Limouri (github) ‚Üî Hamza LIMOURI (linkedin) | Loc: morocco / other\n",
      "14. [0.528] Aimad SADOUK (github) ‚Üî Aimad SADOUK (linkedin) | Loc: morocco / other\n",
      "15. [0.610] Oussama RAJI (github) ‚Üî Oussama K (linkedin) | Loc: morocco / other\n",
      "16. [0.538] Ismail Tarik (github) ‚Üî ISMAIL LARHCHIM (linkedin) | Loc: morocco / other\n",
      "17. [0.560] Younes Bousetta (github) ‚Üî Younes Bousetta (linkedin) | Loc: morocco / other\n",
      "18. [0.508] Naouar EL BOUMASHOULI (github) ‚Üî Naouar EL BOUMASHOULI (linkedin) | Loc: morocco / other\n",
      "19. [0.519] Aicha ZEROUAL (github) ‚Üî Aicha El oubaydi (linkedin) | Loc: morocco / other\n",
      "20. [0.512] Mohamed Aqlil (github) ‚Üî Mohamed Aqlil (linkedin) | Loc: morocco / other\n"
     ]
    }
   ],
   "source": [
    "# step3_weighted_matching.py\n",
    "\"\"\"\n",
    "√âTAPE 3 : Matching Final Pond√©r√© ‚Äî VERSION OPTIMIS√âE\n",
    "- Cible ‚â•100 paires fiables\n",
    "- √âvite les faux positifs\n",
    "- Supporte les profils techniques marocains (ex: Omar MHAIMDAT)\n",
    "- Compatible avec step2_semantic_representation.py\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "def normalize_email(email):\n",
    "    if not email or \"@\" not in str(email):\n",
    "        return None\n",
    "    return str(email).strip().lower()\n",
    "\n",
    "def cosine_sim(u, v):\n",
    "    norm_u = np.linalg.norm(u)\n",
    "    norm_v = np.linalg.norm(v)\n",
    "    if norm_u == 0 or norm_v == 0:\n",
    "        return 0.0\n",
    "    return np.dot(u, v) / (norm_u * norm_v + 1e-8)\n",
    "\n",
    "def jaro_winkler(s1, s2):\n",
    "    if not s1 or not s2:\n",
    "        return 0.0\n",
    "    s1, s2 = s1.lower(), s2.lower()\n",
    "    if s1 == s2:\n",
    "        return 1.0\n",
    "    len1, len2 = len(s1), len(s2)\n",
    "    match_window = max(len1, len2) // 2 - 1\n",
    "    match_window = max(0, match_window)\n",
    "\n",
    "    s1_matches = [False] * len1\n",
    "    s2_matches = [False] * len2\n",
    "    matches = 0\n",
    "    transpositions = 0\n",
    "\n",
    "    for i in range(len1):\n",
    "        start = max(0, i - match_window)\n",
    "        end = min(i + match_window + 1, len2)\n",
    "        for j in range(start, end):\n",
    "            if s2_matches[j] or s1[i] != s2[j]:\n",
    "                continue\n",
    "            s1_matches[i] = s2_matches[j] = True\n",
    "            matches += 1\n",
    "            break\n",
    "\n",
    "    if matches == 0:\n",
    "        return 0.0\n",
    "\n",
    "    k = 0\n",
    "    for i in range(len1):\n",
    "        if not s1_matches[i]:\n",
    "            continue\n",
    "        while not s2_matches[k]:\n",
    "            k += 1\n",
    "        if s1[i] != s2[k]:\n",
    "            transpositions += 1\n",
    "        k += 1\n",
    "\n",
    "    jaro = (matches/len1 + matches/len2 + (matches - transpositions/2)/matches) / 3.0\n",
    "    prefix = 0\n",
    "    for i in range(min(4, min(len1, len2))):\n",
    "        if s1[i] == s2[i]:\n",
    "            prefix += 1\n",
    "        else:\n",
    "            break\n",
    "    return jaro + (prefix * 0.1 * (1 - jaro))\n",
    "\n",
    "def extract_first_last(name):\n",
    "    if not name or not isinstance(name, str):\n",
    "        return \"\", \"\"\n",
    "    parts = name.strip().split()\n",
    "    if not parts:\n",
    "        return \"\", \"\"\n",
    "    first = parts[0].lower()\n",
    "    last = parts[-1].lower() if len(parts) > 1 else \"\"\n",
    "    return first, last\n",
    "\n",
    "# -----------------------------\n",
    "# Seuils de base\n",
    "# -----------------------------\n",
    "THRESHOLDS = {\n",
    "    (\"github\", \"linkedin\"): 0.55,\n",
    "    (\"linkedin\", \"github\"): 0.55,\n",
    "    (\"github\", \"twitter\"): 0.50,\n",
    "    (\"twitter\", \"github\"): 0.50,\n",
    "    (\"linkedin\", \"twitter\"): 0.50,\n",
    "    (\"twitter\", \"linkedin\"): 0.50,\n",
    "}\n",
    "\n",
    "def get_dynamic_threshold(p1, p2):\n",
    "    key = (p1[\"platform\"], p2[\"platform\"])\n",
    "    base_thresh = THRESHOLDS.get(key, 0.50)\n",
    "\n",
    "    full_name_lex = jaro_winkler(p1.get(\"fullName\", \"\"), p2.get(\"fullName\", \"\"))\n",
    "    if full_name_lex >= 0.85:\n",
    "        return max(0.45, base_thresh - 0.05)\n",
    "\n",
    "    # Liens explicites ou email identique ‚Üí seuil tr√®s bas\n",
    "    if (p1[\"email_norm\"] and p2[\"email_norm\"] and p1[\"email_norm\"] == p2[\"email_norm\"]) or \\\n",
    "       (p1[\"platform\"] == \"github\" and p2.get(\"username\") and p2[\"username\"].lower() in (p1.get(\"bio\", \"\") + \" \" + p1.get(\"repo_descriptions\", \"\")).lower()) or \\\n",
    "       (p2[\"platform\"] == \"github\" and p1.get(\"username\") and p1[\"username\"].lower() in (p2.get(\"bio\", \"\") + \" \" + p2.get(\"repo_descriptions\", \"\")).lower()):\n",
    "        return 0.40\n",
    "\n",
    "    return base_thresh\n",
    "\n",
    "def is_likely_false_positive(p1, p2, score):\n",
    "    name1_parts = p1.get(\"fullName\", \"\").strip().split()\n",
    "    name2_parts = p2.get(\"fullName\", \"\").strip().split()\n",
    "    if not name1_parts or not name2_parts:\n",
    "        return True\n",
    "\n",
    "    last1 = name1_parts[-1].lower()\n",
    "    last2 = name2_parts[-1].lower()\n",
    "\n",
    "    # Si noms de famille tr√®s diff√©rents ET pas de lien fort\n",
    "    if last1 != last2 and score < 0.60:\n",
    "        gh_bio = \"\"\n",
    "        other_user = \"\"\n",
    "        if p1[\"platform\"] == \"github\":\n",
    "            gh_bio = (p1.get(\"bio\", \"\") + \" \" + p1.get(\"repo_descriptions\", \"\")).lower()\n",
    "            other_user = p2.get(\"username\", \"\").lower()\n",
    "        elif p2[\"platform\"] == \"github\":\n",
    "            gh_bio = (p2.get(\"bio\", \"\") + \" \" + p2.get(\"repo_descriptions\", \"\")).lower()\n",
    "            other_user = p1.get(\"username\", \"\").lower()\n",
    "        # Pas de lien explicite ni email commun ?\n",
    "        if not (other_user and other_user in gh_bio) and not (p1[\"email_norm\"] == p2[\"email_norm\"]):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def main():\n",
    "    print(\"üöÄ √âTAPE 3 : Matching Final Pond√©r√© ‚Äî OPTIMIS√â POUR ‚â•100 PAIRES\")\n",
    "    print(\"   ‚Üí Ciblage des profils tech marocains (ex: Omar MHAIMDAT)\")\n",
    "    print(\"   ‚Üí √âquilibre rappel/pr√©cision\")\n",
    "\n",
    "    output_dir = Path(\"output\")\n",
    "    with open(output_dir / \"profiles_metadata.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        profiles = json.load(f)\n",
    "    n = len(profiles)\n",
    "\n",
    "    # Charger embeddings\n",
    "    field_embeddings = {}\n",
    "    for field in [\"fullName\", \"username\", \"bio\", \"repo_descriptions\", \"headline\"]:\n",
    "        field_embeddings[field] = np.load(output_dir / f\"{field}_embeddings.npy\")\n",
    "\n",
    "    # Enrichir profils\n",
    "    for p in profiles:\n",
    "        p[\"email_norm\"] = normalize_email(p.get(\"email\"))\n",
    "        p[\"first\"], p[\"last\"] = extract_first_last(p.get(\"fullName\", \"\"))\n",
    "\n",
    "    # Indexation avanc√©e\n",
    "    email_to_idx = defaultdict(list)\n",
    "    first_name_to_idx = defaultdict(list)\n",
    "    blocking_key_to_idx = defaultdict(list)\n",
    "    username_to_idx = defaultdict(list)\n",
    "    location_to_idx = defaultdict(list)  # ‚Üê ajout pour profils marocains\n",
    "\n",
    "    moroccan_cities = {\"casablanca\", \"rabat\", \"mohammedia\", \"marrakech\", \"fes\", \"agadir\", \"tanger\", \"meknes\", \"oujda\", \"kenitra\"}\n",
    "\n",
    "    for i, p in enumerate(profiles):\n",
    "        loc = str(p.get(\"location\", \"\")).lower()\n",
    "        is_moroccan = \"morocco\" in loc or \"maroc\" in loc or any(city in loc for city in moroccan_cities)\n",
    "\n",
    "        if p[\"email_norm\"]:\n",
    "            email_to_idx[p[\"email_norm\"]].append(i)\n",
    "        if p[\"first\"]:\n",
    "            first_name_to_idx[p[\"first\"]].append(i)\n",
    "            if is_moroccan:\n",
    "                location_to_idx[(p[\"first\"], \"morocco\")].append(i)\n",
    "        key = (p[\"first\"][:2], p[\"last\"][0] if p[\"last\"] else \"\")\n",
    "        blocking_key_to_idx[key].append(i)\n",
    "        if p.get(\"username\"):\n",
    "            username_to_idx[p[\"username\"].lower()].append(i)\n",
    "\n",
    "    def compute_score(i, j):\n",
    "        p1, p2 = profiles[i], profiles[j]\n",
    "        platform1, platform2 = p1[\"platform\"], p2[\"platform\"]\n",
    "        score = 0.0\n",
    "\n",
    "        # 1. Email exact ‚Üí 0.30\n",
    "        if p1[\"email_norm\"] and p2[\"email_norm\"] and p1[\"email_norm\"] == p2[\"email_norm\"]:\n",
    "            score += 0.30\n",
    "\n",
    "        # 2. Liens externes (GitHub bio contient username)\n",
    "        gh = p1 if p1[\"platform\"] == \"github\" else (p2 if p2[\"platform\"] == \"github\" else None)\n",
    "        other = p2 if gh is p1 else (p1 if gh is p2 else None)\n",
    "        if gh and other:\n",
    "            gh_text = (gh.get(\"bio\", \"\") + \" \" + gh.get(\"repo_descriptions\", \"\")).lower()\n",
    "            other_user = other.get(\"username\", \"\").lower()\n",
    "            if other_user and other_user in gh_text:\n",
    "                score += 0.30\n",
    "\n",
    "        # 3. fullName ‚Üí 0.25\n",
    "        score += 0.25 * cosine_sim(field_embeddings[\"fullName\"][i], field_embeddings[\"fullName\"][j])\n",
    "\n",
    "        # 4. username ‚Üí 0.10\n",
    "        score += 0.10 * cosine_sim(field_embeddings[\"username\"][i], field_embeddings[\"username\"][j])\n",
    "\n",
    "        # 5. bio ‚Üí 0.05\n",
    "        score += 0.05 * cosine_sim(field_embeddings[\"bio\"][i], field_embeddings[\"bio\"][j])\n",
    "\n",
    "        # 6. repo ‚Üî headline (GitHub ‚Üî LinkedIn)\n",
    "        if (platform1 == \"github\" and platform2 == \"linkedin\"):\n",
    "            score += 0.30 * cosine_sim(field_embeddings[\"repo_descriptions\"][i], field_embeddings[\"headline\"][j])\n",
    "        elif (platform1 == \"linkedin\" and platform2 == \"github\"):\n",
    "            score += 0.30 * cosine_sim(field_embeddings[\"repo_descriptions\"][j], field_embeddings[\"headline\"][i])\n",
    "\n",
    "        return min(1.0, score)\n",
    "\n",
    "    candidate_pairs = []\n",
    "\n",
    "    for i in range(n):\n",
    "        p = profiles[i]\n",
    "        candidates = set()\n",
    "\n",
    "        if p[\"email_norm\"]:\n",
    "            candidates.update(email_to_idx[p[\"email_norm\"]])\n",
    "        if p[\"first\"]:\n",
    "            candidates.update(first_name_to_idx[p[\"first\"]])\n",
    "            # Ajout cibl√© pour profils marocains\n",
    "            candidates.update(location_to_idx.get((p[\"first\"], \"morocco\"), []))\n",
    "        key = (p[\"first\"][:2], p[\"last\"][0] if p[\"last\"] else \"\")\n",
    "        candidates.update(blocking_key_to_idx[key])\n",
    "        if p[\"platform\"] == \"github\":\n",
    "            gh_text = (p.get(\"bio\", \"\") + \" \" + p.get(\"repo_descriptions\", \"\")).lower()\n",
    "            for j in range(n):\n",
    "                if i == j or profiles[j][\"platform\"] == \"github\":\n",
    "                    continue\n",
    "                other_user = profiles[j].get(\"username\", \"\").lower()\n",
    "                if other_user and other_user in gh_text:\n",
    "                    candidates.add(j)\n",
    "\n",
    "        for j in candidates:\n",
    "            if i >= j or profiles[i][\"platform\"] == profiles[j][\"platform\"]:\n",
    "                continue\n",
    "\n",
    "            p1, p2 = profiles[i], profiles[j]\n",
    "            if p1[\"first\"] != p2[\"first\"]:\n",
    "                continue\n",
    "\n",
    "            # Signaux de confiance obligatoires\n",
    "            full_name_cos = cosine_sim(field_embeddings[\"fullName\"][i], field_embeddings[\"fullName\"][j])\n",
    "            full_name_lex = jaro_winkler(p1.get(\"fullName\", \"\"), p2.get(\"fullName\", \"\"))\n",
    "\n",
    "            has_trusted_signal = (\n",
    "                (p1[\"email_norm\"] and p2[\"email_norm\"] and p1[\"email_norm\"] == p2[\"email_norm\"]) or\n",
    "                (p1[\"platform\"] == \"github\" and p2.get(\"username\") and p2[\"username\"].lower() in (p1.get(\"bio\", \"\") + \" \" + p1.get(\"repo_descriptions\", \"\")).lower()) or\n",
    "                (p2[\"platform\"] == \"github\" and p1.get(\"username\") and p1[\"username\"].lower() in (p2.get(\"bio\", \"\") + \" \" + p2.get(\"repo_descriptions\", \"\")).lower()) or\n",
    "                (full_name_lex >= 0.80) or\n",
    "                (full_name_cos >= 0.75)\n",
    "            )\n",
    "            if not has_trusted_signal:\n",
    "                continue\n",
    "\n",
    "            score = compute_score(i, j)\n",
    "            dynamic_thresh = get_dynamic_threshold(p1, p2)\n",
    "            if score >= dynamic_thresh:\n",
    "                if not is_likely_false_positive(p1, p2, score):\n",
    "                    candidate_pairs.append((i, j, score))\n",
    "\n",
    "    # Matching 1:1 strict\n",
    "    best_match = {}\n",
    "    for i, j, score in candidate_pairs:\n",
    "        if i not in best_match or best_match[i][1] < score:\n",
    "            best_match[i] = (j, score)\n",
    "        if j not in best_match or best_match[j][1] < score:\n",
    "            best_match[j] = (i, score)\n",
    "\n",
    "    final_matches = []\n",
    "    used = set()\n",
    "    for i, j, score in candidate_pairs:\n",
    "        if i in used or j in used:\n",
    "            continue\n",
    "        if best_match.get(i) == (j, score) and best_match.get(j) == (i, score):\n",
    "            final_matches.append((i, j, score))\n",
    "            used.add(i)\n",
    "            used.add(j)\n",
    "\n",
    "    # Transitivit√© (Union-Find)\n",
    "    parent = list(range(n))\n",
    "    def find(x):\n",
    "        if parent[x] != x:\n",
    "            parent[x] = find(parent[x])\n",
    "        return parent[x]\n",
    "    def union(x, y):\n",
    "        rx, ry = find(x), find(y)\n",
    "        if rx != ry:\n",
    "            parent[ry] = rx\n",
    "\n",
    "    for i, j, _ in final_matches:\n",
    "        union(i, j)\n",
    "\n",
    "    # Clusters unifi√©s\n",
    "    components = defaultdict(list)\n",
    "    for i in range(n):\n",
    "        root = find(i)\n",
    "        components[root].append(i)\n",
    "\n",
    "    unified = []\n",
    "    for comp in components.values():\n",
    "        unified.append({\n",
    "            \"unified_id\": f\"person_{len(unified):05d}\",\n",
    "            \"profiles\": [profiles[i] for i in comp]\n",
    "        })\n",
    "\n",
    "    with open(output_dir / \"unified_profiles.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(unified, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"\\n‚úÖ {len(final_matches)} paires valides trouv√©es\")\n",
    "    print(f\"‚úÖ {len(unified)} identit√©s unifi√©es (sur {n} profils initiaux)\")\n",
    "    print(\"‚úÖ R√©sultats sauvegard√©s dans 'output/unified_profiles.json'\")\n",
    "\n",
    "    # üîç Affichage des matches\n",
    "    print(\"\\nüîç Top 20 paires trouv√©es :\")\n",
    "    for idx, (i, j, score) in enumerate(final_matches[:20], 1):\n",
    "        p1, p2 = profiles[i], profiles[j]\n",
    "        print(f\"{idx:2d}. [{score:.3f}] \"\n",
    "              f\"{p1.get('fullName', 'N/A')} ({p1['platform']}) ‚Üî \"\n",
    "              f\"{p2.get('fullName', 'N/A')} ({p2['platform']}) | \"\n",
    "              f\"Loc: {p1.get('location', '')} / {p2.get('location', '')}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c3a5924-3279-447d-bb58-b5938c371e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Nombre d'identit√©s unifi√©es incluant Twitter + autre plateforme : 37\n",
      "\n",
      "üîç Exemples de profils unifi√©s avec Twitter :\n",
      "1. {'Yassir Acharki'} ‚Üí plateformes: ['github', 'twitter']\n",
      "2. {'Yasser', 'Yasser Chenik'} ‚Üí plateformes: ['github', 'twitter']\n",
      "3. {'Rida', 'RIDA BELMOUDEN'} ‚Üí plateformes: ['github', 'twitter']\n",
      "4. {'ayoub', 'ayoub zaanouni'} ‚Üí plateformes: ['github', 'twitter']\n",
      "5. {'Zayd inani', 'zayd inani'} ‚Üí plateformes: ['github', 'twitter']\n",
      "6. {'Saad Daali', 'Saad'} ‚Üí plateformes: ['github', 'twitter']\n",
      "7. {'Aymane Benhima', 'AYMANE BENHIMA'} ‚Üí plateformes: ['github', 'twitter']\n",
      "8. {'Khalid Jaafary', 'Khalid'} ‚Üí plateformes: ['github', 'twitter']\n",
      "9. {'mo', 'Mo'} ‚Üí plateformes: ['github', 'twitter']\n",
      "10. {'Mohamed amallaz', 'Mohamed'} ‚Üí plateformes: ['github', 'twitter']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def main():\n",
    "    output_dir = Path(\"output\")\n",
    "    with open(output_dir / \"unified_profiles.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        unified = json.load(f)\n",
    "\n",
    "    twitter_matches = []\n",
    "    for person in unified:\n",
    "        platforms = {p[\"platform\"] for p in person[\"profiles\"]}\n",
    "        if \"twitter\" in platforms and len(person[\"profiles\"]) >= 2:\n",
    "            # Au moins Twitter + une autre plateforme\n",
    "            twitter_matches.append(person)\n",
    "\n",
    "    print(f\"‚úÖ Nombre d'identit√©s unifi√©es incluant Twitter + autre plateforme : {len(twitter_matches)}\")\n",
    "    \n",
    "    if twitter_matches:\n",
    "        print(\"\\nüîç Exemples de profils unifi√©s avec Twitter :\")\n",
    "        for i, person in enumerate(twitter_matches[:10], 1):\n",
    "            names = [p.get(\"fullName\", \"N/A\") for p in person[\"profiles\"]]\n",
    "            platforms = [p[\"platform\"] for p in person[\"profiles\"]]\n",
    "            print(f\"{i}. {set(names)} ‚Üí plateformes: {platforms}\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Aucune identit√© unifi√©e ne contient Twitter avec une autre plateforme.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86dfc703-ee5f-4754-b6f1-56d60cfe5851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Nombre d'identit√©s unifi√©es avec Twitter + LinkedIn : 0\n",
      "\n",
      "‚ùå Aucune identit√© unifi√©e ne contient √† la fois Twitter et LinkedIn.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def main():\n",
    "    output_dir = Path(\"output\")\n",
    "    with open(output_dir / \"unified_profiles.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        unified = json.load(f)\n",
    "\n",
    "    twitter_linkedin_matches = []\n",
    "\n",
    "    for person in unified:\n",
    "        platforms = {p[\"platform\"] for p in person[\"profiles\"]}\n",
    "        if \"twitter\" in platforms and \"linkedin\" in platforms:\n",
    "            twitter_linkedin_matches.append(person)\n",
    "\n",
    "    print(f\"‚úÖ Nombre d'identit√©s unifi√©es avec Twitter + LinkedIn : {len(twitter_linkedin_matches)}\")\n",
    "    \n",
    "    if twitter_linkedin_matches:\n",
    "        print(\"\\nüîç D√©tail des paires Twitter ‚Üî LinkedIn :\")\n",
    "        for i, person in enumerate(twitter_linkedin_matches, 1):\n",
    "            tw_profile = next(p for p in person[\"profiles\"] if p[\"platform\"] == \"twitter\")\n",
    "            li_profile = next(p for p in person[\"profiles\"] if p[\"platform\"] == \"linkedin\")\n",
    "            \n",
    "            tw_name = tw_profile.get(\"fullName\", \"N/A\")\n",
    "            li_name = li_profile.get(\"fullName\", \"N/A\")\n",
    "            tw_user = tw_profile.get(\"username\", \"N/A\")\n",
    "            li_headline = li_profile.get(\"headline\", \"\")[:60]\n",
    "            location = tw_profile.get(\"location\", \"\") or li_profile.get(\"location\", \"N/A\")\n",
    "\n",
    "            print(f\"{i:2d}. Twitter: {tw_name} (@{tw_user})\")\n",
    "            print(f\"    LinkedIn: {li_name} | {li_headline}...\")\n",
    "            print(f\"    üìç {location}\")\n",
    "            print(\"-\" * 60)\n",
    "    else:\n",
    "        print(\"\\n‚ùå Aucune identit√© unifi√©e ne contient √† la fois Twitter et LinkedIn.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38243d18-1bd1-4f53-874f-4e997127e444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ √âTAPE 3 : Matching Final Pond√©r√© ‚Äî CIBLE ‚â•200 PAIRES SANS FAUX POSITIFS\n",
      "   ‚Üí Ciblage renforc√© des profils tech marocains\n",
      "   ‚Üí Pr√©cision maximale, rappel augment√©\n",
      "\n",
      "‚úÖ 190 paires valides trouv√©es\n",
      "‚úÖ 11209 identit√©s unifi√©es (sur 11399 profils initiaux)\n",
      "‚úÖ R√©sultats sauvegard√©s dans 'output/unified_profiles.json'\n",
      "\n",
      "üîç Top 20 paires trouv√©es :\n",
      " 1. [0.528] IKRAM (github) ‚Üî Ikram Daoudi (linkedin) | Loc: morocco / other\n",
      " 2. [0.532] Youness (github) ‚Üî Youness A (linkedin) | Loc: morocco / morocco\n",
      " 3. [0.762] Taibi EL Yakouti (github) ‚Üî Taibi El Yakouti (linkedin) | Loc: morocco / other\n",
      " 4. [0.505] Adama COULIBALY (github) ‚Üî Adama Ndiaye (linkedin) | Loc: morocco / morocco\n",
      " 5. [0.547] Lamiae Hana (github) ‚Üî Lamiae Hana (linkedin) | Loc: morocco / other\n",
      " 6. [0.504] Hamza OKHADIR (github) ‚Üî Hamza Lghali (linkedin) | Loc: morocco / morocco\n",
      " 7. [0.518] Ayoub Najjout (github) ‚Üî Ayoub Najjout (linkedin) | Loc: morocco / other\n",
      " 8. [0.505] Boutaina ELYAZIJI (github) ‚Üî Boutaina ELYAZIJI (linkedin) | Loc: morocco / other\n",
      " 9. [0.502] Abdelmoughit ASSAL (github) ‚Üî Abdelmoughit Assal (linkedin) | Loc: morocco / other\n",
      "10. [0.671] Yassir Acharki (github) ‚Üî Yassir Acharki (twitter) | Loc: morocco / other\n",
      "11. [0.699] Brahim Alaoui (github) ‚Üî Brahim AADIL (linkedin) | Loc: morocco / morocco\n",
      "12. [0.663] Khadija Mouhtaj (github) ‚Üî Khadija Mekouar (linkedin) | Loc: morocco / other\n",
      "13. [0.530] Bouarfa Lahmar (github) ‚Üî Bouarfa Lahmar (linkedin) | Loc: morocco / other\n",
      "14. [0.525] Mohammed Nabil (github) ‚Üî Mohammed Nabil (linkedin) | Loc: morocco / other\n",
      "15. [0.547] Hicham Nouhaidi (github) ‚Üî Hicham Ouchri (linkedin) | Loc: morocco / morocco\n",
      "16. [0.506] Hamza Limouri (github) ‚Üî Hamza LIMOURI (linkedin) | Loc: morocco / other\n",
      "17. [0.532] Aimad SADOUK (github) ‚Üî Aimad Sabour (linkedin) | Loc: morocco / morocco\n",
      "18. [0.571] Abdellah Ennajari (github) ‚Üî Abdellah Ennajari (linkedin) | Loc: morocco / morocco\n",
      "19. [0.610] Oussama RAJI (github) ‚Üî Oussama K (linkedin) | Loc: morocco / other\n",
      "20. [0.565] Ismail Tarik (github) ‚Üî ismail lamchandaq (linkedin) | Loc: morocco / morocco\n"
     ]
    }
   ],
   "source": [
    "# step3_weighted_matching.py\n",
    "\"\"\"\n",
    "√âTAPE 3 : Matching Final Pond√©r√© ‚Äî CIBLE ‚â•200 PAIRES SANS FAUX POSITIFS\n",
    "- √âvite les faux positifs √† tout prix\n",
    "- Supporte les profils techniques marocains (ex: Omar MHAIMDAT, Oumayma El Ghizlani)\n",
    "- Compatible avec step2_semantic_representation.py\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "def normalize_email(email):\n",
    "    if not email or \"@\" not in str(email):\n",
    "        return None\n",
    "    return str(email).strip().lower()\n",
    "\n",
    "def cosine_sim(u, v):\n",
    "    norm_u = np.linalg.norm(u)\n",
    "    norm_v = np.linalg.norm(v)\n",
    "    if norm_u == 0 or norm_v == 0:\n",
    "        return 0.0\n",
    "    return np.dot(u, v) / (norm_u * norm_v + 1e-8)\n",
    "\n",
    "def jaro_winkler(s1, s2):\n",
    "    if not s1 or not s2:\n",
    "        return 0.0\n",
    "    s1, s2 = s1.lower(), s2.lower()\n",
    "    if s1 == s2:\n",
    "        return 1.0\n",
    "    len1, len2 = len(s1), len(s2)\n",
    "    match_window = max(len1, len2) // 2 - 1\n",
    "    match_window = max(0, match_window)\n",
    "\n",
    "    s1_matches = [False] * len1\n",
    "    s2_matches = [False] * len2\n",
    "    matches = 0\n",
    "    transpositions = 0\n",
    "\n",
    "    for i in range(len1):\n",
    "        start = max(0, i - match_window)\n",
    "        end = min(i + match_window + 1, len2)\n",
    "        for j in range(start, end):\n",
    "            if s2_matches[j] or s1[i] != s2[j]:\n",
    "                continue\n",
    "            s1_matches[i] = s2_matches[j] = True\n",
    "            matches += 1\n",
    "            break\n",
    "\n",
    "    if matches == 0:\n",
    "        return 0.0\n",
    "\n",
    "    k = 0\n",
    "    for i in range(len1):\n",
    "        if not s1_matches[i]:\n",
    "            continue\n",
    "        while not s2_matches[k]:\n",
    "            k += 1\n",
    "        if s1[i] != s2[k]:\n",
    "            transpositions += 1\n",
    "        k += 1\n",
    "\n",
    "    jaro = (matches/len1 + matches/len2 + (matches - transpositions/2)/matches) / 3.0\n",
    "    prefix = 0\n",
    "    for i in range(min(4, min(len1, len2))):\n",
    "        if s1[i] == s2[i]:\n",
    "            prefix += 1\n",
    "        else:\n",
    "            break\n",
    "    return jaro + (prefix * 0.1 * (1 - jaro))\n",
    "\n",
    "def extract_first_last(name):\n",
    "    if not name or not isinstance(name, str):\n",
    "        return \"\", \"\"\n",
    "    parts = name.strip().split()\n",
    "    if not parts:\n",
    "        return \"\", \"\"\n",
    "    first = parts[0].lower()\n",
    "    last = parts[-1].lower() if len(parts) > 1 else \"\"\n",
    "    return first, last\n",
    "\n",
    "# -----------------------------\n",
    "# Seuils de base\n",
    "# -----------------------------\n",
    "THRESHOLDS = {\n",
    "    (\"github\", \"linkedin\"): 0.55,\n",
    "    (\"linkedin\", \"github\"): 0.55,\n",
    "    (\"github\", \"twitter\"): 0.50,\n",
    "    (\"twitter\", \"github\"): 0.50,\n",
    "    (\"linkedin\", \"twitter\"): 0.50,\n",
    "    (\"twitter\", \"linkedin\"): 0.50,\n",
    "}\n",
    "\n",
    "def get_dynamic_threshold(p1, p2):\n",
    "    key = (p1[\"platform\"], p2[\"platform\"])\n",
    "    base_thresh = THRESHOLDS.get(key, 0.50)\n",
    "\n",
    "    full_name_lex = jaro_winkler(p1.get(\"fullName\", \"\"), p2.get(\"fullName\", \"\"))\n",
    "    if full_name_lex >= 0.85:\n",
    "        return max(0.45, base_thresh - 0.05)\n",
    "\n",
    "    # Liens explicites ou email identique ‚Üí seuil tr√®s bas\n",
    "    if (p1[\"email_norm\"] and p2[\"email_norm\"] and p1[\"email_norm\"] == p2[\"email_norm\"]) or \\\n",
    "       (p1[\"platform\"] == \"github\" and p2.get(\"username\") and p2[\"username\"].lower() in (p1.get(\"bio\", \"\") + \" \" + p1.get(\"repo_descriptions\", \"\")).lower()) or \\\n",
    "       (p2[\"platform\"] == \"github\" and p1.get(\"username\") and p1[\"username\"].lower() in (p2.get(\"bio\", \"\") + \" \" + p2.get(\"repo_descriptions\", \"\")).lower()):\n",
    "        return 0.40\n",
    "\n",
    "    return base_thresh\n",
    "\n",
    "def is_likely_false_positive(p1, p2, score):\n",
    "    name1_parts = p1.get(\"fullName\", \"\").strip().split()\n",
    "    name2_parts = p2.get(\"fullName\", \"\").strip().split()\n",
    "    if not name1_parts or not name2_parts:\n",
    "        return True\n",
    "\n",
    "    last1 = name1_parts[-1].lower()\n",
    "    last2 = name2_parts[-1].lower()\n",
    "\n",
    "    # Si noms de famille tr√®s diff√©rents ET pas de lien fort\n",
    "    if last1 != last2 and score < 0.60:\n",
    "        gh_bio = \"\"\n",
    "        other_user = \"\"\n",
    "        if p1[\"platform\"] == \"github\":\n",
    "            gh_bio = (p1.get(\"bio\", \"\") + \" \" + p1.get(\"repo_descriptions\", \"\")).lower()\n",
    "            other_user = p2.get(\"username\", \"\").lower()\n",
    "        elif p2[\"platform\"] == \"github\":\n",
    "            gh_bio = (p2.get(\"bio\", \"\") + \" \" + p2.get(\"repo_descriptions\", \"\")).lower()\n",
    "            other_user = p1.get(\"username\", \"\").lower()\n",
    "        # Pas de lien explicite ni email commun ?\n",
    "        if not (other_user and other_user in gh_bio) and not (p1[\"email_norm\"] == p2[\"email_norm\"]):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def main():\n",
    "    print(\"üöÄ √âTAPE 3 : Matching Final Pond√©r√© ‚Äî CIBLE ‚â•200 PAIRES SANS FAUX POSITIFS\")\n",
    "    print(\"   ‚Üí Ciblage renforc√© des profils tech marocains\")\n",
    "    print(\"   ‚Üí Pr√©cision maximale, rappel augment√©\")\n",
    "\n",
    "    output_dir = Path(\"output\")\n",
    "    with open(output_dir / \"profiles_metadata.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        profiles = json.load(f)\n",
    "    n = len(profiles)\n",
    "\n",
    "    # Charger embeddings\n",
    "    field_embeddings = {}\n",
    "    for field in [\"fullName\", \"username\", \"bio\", \"repo_descriptions\", \"headline\"]:\n",
    "        field_embeddings[field] = np.load(output_dir / f\"{field}_embeddings.npy\")\n",
    "\n",
    "    # Enrichir profils\n",
    "    for p in profiles:\n",
    "        p[\"email_norm\"] = normalize_email(p.get(\"email\"))\n",
    "        p[\"first\"], p[\"last\"] = extract_first_last(p.get(\"fullName\", \"\"))\n",
    "\n",
    "    # Indexation avanc√©e\n",
    "    email_to_idx = defaultdict(list)\n",
    "    first_name_to_idx = defaultdict(list)\n",
    "    blocking_key_to_idx = defaultdict(list)\n",
    "    username_to_idx = defaultdict(list)\n",
    "    location_to_idx = defaultdict(list)\n",
    "\n",
    "    moroccan_cities = {\"casablanca\", \"rabat\", \"mohammedia\", \"marrakech\", \"fes\", \"agadir\", \"tanger\", \"meknes\", \"oujda\", \"kenitra\"}\n",
    "\n",
    "    for i, p in enumerate(profiles):\n",
    "        loc = str(p.get(\"location\", \"\")).lower()\n",
    "        is_moroccan = \"morocco\" in loc or \"maroc\" in loc or any(city in loc for city in moroccan_cities)\n",
    "\n",
    "        if p[\"email_norm\"]:\n",
    "            email_to_idx[p[\"email_norm\"]].append(i)\n",
    "        if p[\"first\"]:\n",
    "            first_name_to_idx[p[\"first\"]].append(i)\n",
    "            if is_moroccan:\n",
    "                location_to_idx[(p[\"first\"], \"morocco\")].append(i)\n",
    "        key = (p[\"first\"][:2], p[\"last\"][0] if p[\"last\"] else \"\")\n",
    "        blocking_key_to_idx[key].append(i)\n",
    "        if p.get(\"username\"):\n",
    "            username_to_idx[p[\"username\"].lower()].append(i)\n",
    "\n",
    "    def compute_score(i, j):\n",
    "        p1, p2 = profiles[i], profiles[j]\n",
    "        platform1, platform2 = p1[\"platform\"], p2[\"platform\"]\n",
    "        score = 0.0\n",
    "\n",
    "        # 1. Email exact ‚Üí 0.30\n",
    "        if p1[\"email_norm\"] and p2[\"email_norm\"] and p1[\"email_norm\"] == p2[\"email_norm\"]:\n",
    "            score += 0.30\n",
    "\n",
    "        # 2. Liens externes (GitHub bio contient username)\n",
    "        gh = p1 if p1[\"platform\"] == \"github\" else (p2 if p2[\"platform\"] == \"github\" else None)\n",
    "        other = p2 if gh is p1 else (p1 if gh is p2 else None)\n",
    "        if gh and other:\n",
    "            gh_text = (gh.get(\"bio\", \"\") + \" \" + gh.get(\"repo_descriptions\", \"\")).lower()\n",
    "            other_user = other.get(\"username\", \"\").lower()\n",
    "            if other_user and other_user in gh_text:\n",
    "                score += 0.30\n",
    "\n",
    "        # 3. fullName ‚Üí 0.25\n",
    "        score += 0.25 * cosine_sim(field_embeddings[\"fullName\"][i], field_embeddings[\"fullName\"][j])\n",
    "\n",
    "        # 4. username ‚Üí 0.10\n",
    "        score += 0.10 * cosine_sim(field_embeddings[\"username\"][i], field_embeddings[\"username\"][j])\n",
    "\n",
    "        # 3b. Bonus username identique (GitHub ‚Üî Twitter)\n",
    "        if {p1[\"platform\"], p2[\"platform\"]} == {\"github\", \"twitter\"}:\n",
    "            u1 = p1.get(\"username\", \"\").lower()\n",
    "            u2 = p2.get(\"username\", \"\").lower()\n",
    "            if u1 and u2 and u1 == u2:\n",
    "                score += 0.20\n",
    "\n",
    "        # 5. bio ‚Üí 0.05\n",
    "        score += 0.05 * cosine_sim(field_embeddings[\"bio\"][i], field_embeddings[\"bio\"][j])\n",
    "\n",
    "        # 6. repo ‚Üî headline (GitHub ‚Üî LinkedIn)\n",
    "        if (platform1 == \"github\" and platform2 == \"linkedin\"):\n",
    "            score += 0.30 * cosine_sim(field_embeddings[\"repo_descriptions\"][i], field_embeddings[\"headline\"][j])\n",
    "        elif (platform1 == \"linkedin\" and platform2 == \"github\"):\n",
    "            score += 0.30 * cosine_sim(field_embeddings[\"repo_descriptions\"][j], field_embeddings[\"headline\"][i])\n",
    "\n",
    "        # 7. Bonus localisation Maroc + pr√©nom commun + nom partiellement similaire\n",
    "        def is_moroccan(loc):\n",
    "            if not loc:\n",
    "                return False\n",
    "            loc = str(loc).lower()\n",
    "            return \"morocco\" in loc or \"maroc\" in loc or any(city in loc for city in moroccan_cities)\n",
    "\n",
    "        loc1, loc2 = p1.get(\"location\"), p2.get(\"location\")\n",
    "        if p1[\"first\"] == p2[\"first\"] and is_moroccan(loc1) and is_moroccan(loc2):\n",
    "            last_sim = jaro_winkler(p1.get(\"fullName\", \"\"), p2.get(\"fullName\", \"\"))\n",
    "            if last_sim >= 0.65:\n",
    "                score += 0.10\n",
    "\n",
    "        return min(1.0, score)\n",
    "\n",
    "    candidate_pairs = []\n",
    "\n",
    "    for i in range(n):\n",
    "        p = profiles[i]\n",
    "        candidates = set()\n",
    "\n",
    "        if p[\"email_norm\"]:\n",
    "            candidates.update(email_to_idx[p[\"email_norm\"]])\n",
    "        if p[\"first\"]:\n",
    "            candidates.update(first_name_to_idx[p[\"first\"]])\n",
    "            candidates.update(location_to_idx.get((p[\"first\"], \"morocco\"), []))\n",
    "        key = (p[\"first\"][:2], p[\"last\"][0] if p[\"last\"] else \"\")\n",
    "        candidates.update(blocking_key_to_idx[key])\n",
    "        if p[\"platform\"] == \"github\":\n",
    "            gh_text = (p.get(\"bio\", \"\") + \" \" + p.get(\"repo_descriptions\", \"\")).lower()\n",
    "            for j in range(n):\n",
    "                if i == j or profiles[j][\"platform\"] == \"github\":\n",
    "                    continue\n",
    "                other_user = profiles[j].get(\"username\", \"\").lower()\n",
    "                if other_user and other_user in gh_text:\n",
    "                    candidates.add(j)\n",
    "\n",
    "        for j in candidates:\n",
    "            if i >= j or profiles[i][\"platform\"] == profiles[j][\"platform\"]:\n",
    "                continue\n",
    "\n",
    "            p1, p2 = profiles[i], profiles[j]\n",
    "\n",
    "            # üî• Exception : si email identique, on tol√®re un pr√©nom diff√©rent (ex: initiale vs pr√©nom complet)\n",
    "            if p1[\"first\"] != p2[\"first\"]:\n",
    "                if not (p1[\"email_norm\"] and p2[\"email_norm\"] and p1[\"email_norm\"] == p2[\"email_norm\"]):\n",
    "                    continue\n",
    "\n",
    "            # Signaux de confiance obligatoires\n",
    "            full_name_cos = cosine_sim(field_embeddings[\"fullName\"][i], field_embeddings[\"fullName\"][j])\n",
    "            full_name_lex = jaro_winkler(p1.get(\"fullName\", \"\"), p2.get(\"fullName\", \"\"))\n",
    "\n",
    "            has_trusted_signal = (\n",
    "                (p1[\"email_norm\"] and p2[\"email_norm\"] and p1[\"email_norm\"] == p2[\"email_norm\"]) or\n",
    "                (p1[\"platform\"] == \"github\" and p2.get(\"username\") and p2[\"username\"].lower() in (p1.get(\"bio\", \"\") + \" \" + p1.get(\"repo_descriptions\", \"\")).lower()) or\n",
    "                (p2[\"platform\"] == \"github\" and p1.get(\"username\") and p1[\"username\"].lower() in (p2.get(\"bio\", \"\") + \" \" + p2.get(\"repo_descriptions\", \"\")).lower()) or\n",
    "                (full_name_lex >= 0.80) or\n",
    "                (full_name_cos >= 0.75)\n",
    "            )\n",
    "            if not has_trusted_signal:\n",
    "                continue\n",
    "\n",
    "            score = compute_score(i, j)\n",
    "            dynamic_thresh = get_dynamic_threshold(p1, p2)\n",
    "            if score >= dynamic_thresh:\n",
    "                if not is_likely_false_positive(p1, p2, score):\n",
    "                    candidate_pairs.append((i, j, score))\n",
    "\n",
    "    # Matching 1:1 strict\n",
    "    best_match = {}\n",
    "    for i, j, score in candidate_pairs:\n",
    "        if i not in best_match or best_match[i][1] < score:\n",
    "            best_match[i] = (j, score)\n",
    "        if j not in best_match or best_match[j][1] < score:\n",
    "            best_match[j] = (i, score)\n",
    "\n",
    "    final_matches = []\n",
    "    used = set()\n",
    "    for i, j, score in candidate_pairs:\n",
    "        if i in used or j in used:\n",
    "            continue\n",
    "        if best_match.get(i) == (j, score) and best_match.get(j) == (i, score):\n",
    "            final_matches.append((i, j, score))\n",
    "            used.add(i)\n",
    "            used.add(j)\n",
    "\n",
    "    # Transitivit√© (Union-Find)\n",
    "    parent = list(range(n))\n",
    "    def find(x):\n",
    "        if parent[x] != x:\n",
    "            parent[x] = find(parent[x])\n",
    "        return parent[x]\n",
    "    def union(x, y):\n",
    "        rx, ry = find(x), find(y)\n",
    "        if rx != ry:\n",
    "            parent[ry] = rx\n",
    "\n",
    "    for i, j, _ in final_matches:\n",
    "        union(i, j)\n",
    "\n",
    "    # Clusters unifi√©s\n",
    "    components = defaultdict(list)\n",
    "    for i in range(n):\n",
    "        root = find(i)\n",
    "        components[root].append(i)\n",
    "\n",
    "    unified = []\n",
    "    for comp in components.values():\n",
    "        unified.append({\n",
    "            \"unified_id\": f\"person_{len(unified):05d}\",\n",
    "            \"profiles\": [profiles[i] for i in comp]\n",
    "        })\n",
    "\n",
    "    with open(output_dir / \"unified_profiles.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(unified, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"\\n‚úÖ {len(final_matches)} paires valides trouv√©es\")\n",
    "    print(f\"‚úÖ {len(unified)} identit√©s unifi√©es (sur {n} profils initiaux)\")\n",
    "    print(\"‚úÖ R√©sultats sauvegard√©s dans 'output/unified_profiles.json'\")\n",
    "\n",
    "    # üîç Affichage des matches (inclut Twitter)\n",
    "    print(\"\\nüîç Top 20 paires trouv√©es :\")\n",
    "    for idx, (i, j, score) in enumerate(final_matches[:20], 1):\n",
    "        p1, p2 = profiles[i], profiles[j]\n",
    "        print(f\"{idx:2d}. [{score:.3f}] \"\n",
    "              f\"{p1.get('fullName', 'N/A')} ({p1['platform']}) ‚Üî \"\n",
    "              f\"{p2.get('fullName', 'N/A')} ({p2['platform']}) | \"\n",
    "              f\"Loc: {p1.get('location', 'N/A')} / {p2.get('location', 'N/A')}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "780407a6-bd92-40b8-99d7-cf447cda6a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ √âTAPE 3 : Matching Final Pond√©r√© ‚Äî CIBLE ‚â•1000 PAIRES SANS FAUX POSITIFS\n",
      "   ‚Üí Normalisation des noms marocains\n",
      "   ‚Üí Signaux locaux renforc√©s (.ma, villes, variantes)\n",
      "   ‚Üí Coh√©rence s√©mantique obligatoire\n",
      "\n",
      "‚úÖ 441 paires valides trouv√©es\n",
      "‚úÖ 10958 identit√©s unifi√©es (sur 11399 profils initiaux)\n",
      "‚úÖ R√©sultats sauvegard√©s dans 'output/unified_profiles.json'\n",
      "\n",
      "üîç Top 20 paires trouv√©es :\n",
      " 1. [0.529] Omar MHAIMDAT (github) ‚Üî Omar NOUIH (linkedin) | Loc: morocco / morocco\n",
      " 2. [0.495] Anas AIT AOMAR (github) ‚Üî Anas Mokhtari (linkedin) | Loc: morocco / morocco\n",
      " 3. [0.456] SALMA EL BARBORI (github) ‚Üî Salma Bicher (linkedin) | Loc: morocco / morocco\n",
      " 4. [0.536] Hamza Eraoui (github) ‚Üî Hamza Alaoui Ismaili (linkedin) | Loc: morocco / morocco\n",
      " 5. [0.535] hamza DOUAIOUI (github) ‚Üî Hamza AIT ABBOU (linkedin) | Loc: morocco / morocco\n",
      " 6. [0.493] Khalid JOULID (github) ‚Üî Khalid B (linkedin) | Loc: morocco / morocco\n",
      " 7. [0.528] IKRAM (github) ‚Üî Ikram Daoudi (linkedin) | Loc: morocco / other\n",
      " 8. [0.532] Youness (github) ‚Üî Youness A (linkedin) | Loc: morocco / morocco\n",
      " 9. [0.470] Hind FEKKAK (github) ‚Üî Hind FEKKAK (linkedin) | Loc: morocco / other\n",
      "10. [0.468] youssef EL LOUH (github) ‚Üî Youssef EL FILALI (linkedin) | Loc: morocco / morocco\n",
      "11. [0.538] Nouhaila Elhaou (github) ‚Üî Nouhaila Dahmany (linkedin) | Loc: morocco / morocco\n",
      "12. [0.477] Naaima BEN KADOUR (github) ‚Üî Naaima BEN KADOUR (linkedin) | Loc: morocco / other\n",
      "13. [0.473] Abdelilah (github) ‚Üî Abdelilah Makrane (linkedin) | Loc: morocco / morocco\n",
      "14. [0.496] Abdellatif GOU ALI (github) ‚Üî Abdellatif BOUZALIM (linkedin) | Loc: morocco / morocco\n",
      "15. [0.499] Alban NYANTUDRE (github) ‚Üî Alban NYANTUDRE (linkedin) | Loc: morocco / other\n",
      "16. [0.762] Taibi EL Yakouti (github) ‚Üî Taibi El Yakouti (linkedin) | Loc: morocco / other\n",
      "17. [0.483] Safoine El Khabich (github) ‚Üî Safoine El khabich (linkedin) | Loc: morocco / other\n",
      "18. [0.505] Adama COULIBALY (github) ‚Üî Adama Ndiaye (linkedin) | Loc: morocco / morocco\n",
      "19. [0.547] Lamiae Hana (github) ‚Üî Lamiae Hana (linkedin) | Loc: morocco / other\n",
      "20. [0.485] Ayoub El Hadine (github) ‚Üî Ayoub EL HADINE (linkedin) | Loc: morocco / other\n"
     ]
    }
   ],
   "source": [
    "# step3_weighted_matching.py\n",
    "\"\"\"\n",
    "√âTAPE 3 : Matching Final Pond√©r√© ‚Äî CIBLE ‚â•1000 PAIRES SANS FAUX POSITIFS\n",
    "- Exploite les signaux durs (email, liens explicites)\n",
    "- Normalisation des noms marocains/arabes\n",
    "- Coh√©rence s√©mantique et g√©ographique\n",
    "- Supporte les profils techniques marocains (ex: Omar MHAIMDAT, Oumayma El Ghizlani)\n",
    "- Compatible avec step2_semantic_representation.py\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "# === UTILITAIRES ===\n",
    "\n",
    "def normalize_email(email):\n",
    "    if not email or \"@\" not in str(email):\n",
    "        return None\n",
    "    return str(email).strip().lower()\n",
    "\n",
    "def get_email_domain(email):\n",
    "    if email and \"@\" in email:\n",
    "        return email.split(\"@\")[1].lower()\n",
    "    return None\n",
    "\n",
    "def cosine_sim(u, v):\n",
    "    norm_u = np.linalg.norm(u)\n",
    "    norm_v = np.linalg.norm(v)\n",
    "    if norm_u == 0 or norm_v == 0:\n",
    "        return 0.0\n",
    "    return np.dot(u, v) / (norm_u * norm_v + 1e-8)\n",
    "\n",
    "def jaro_winkler(s1, s2):\n",
    "    if not s1 or not s2:\n",
    "        return 0.0\n",
    "    s1, s2 = s1.lower(), s2.lower()\n",
    "    if s1 == s2:\n",
    "        return 1.0\n",
    "    len1, len2 = len(s1), len(s2)\n",
    "    match_window = max(len1, len2) // 2 - 1\n",
    "    match_window = max(0, match_window)\n",
    "\n",
    "    s1_matches = [False] * len1\n",
    "    s2_matches = [False] * len2\n",
    "    matches = 0\n",
    "    transpositions = 0\n",
    "\n",
    "    for i in range(len1):\n",
    "        start = max(0, i - match_window)\n",
    "        end = min(i + match_window + 1, len2)\n",
    "        for j in range(start, end):\n",
    "            if s2_matches[j] or s1[i] != s2[j]:\n",
    "                continue\n",
    "            s1_matches[i] = s2_matches[j] = True\n",
    "            matches += 1\n",
    "            break\n",
    "\n",
    "    if matches == 0:\n",
    "        return 0.0\n",
    "\n",
    "    k = 0\n",
    "    for i in range(len1):\n",
    "        if not s1_matches[i]:\n",
    "            continue\n",
    "        while not s2_matches[k]:\n",
    "            k += 1\n",
    "        if s1[i] != s2[k]:\n",
    "            transpositions += 1\n",
    "        k += 1\n",
    "\n",
    "    jaro = (matches/len1 + matches/len2 + (matches - transpositions/2)/matches) / 3.0\n",
    "    prefix = 0\n",
    "    for i in range(min(4, min(len1, len2))):\n",
    "        if s1[i] == s2[i]:\n",
    "            prefix += 1\n",
    "        else:\n",
    "            break\n",
    "    return jaro + (prefix * 0.1 * (1 - jaro))\n",
    "\n",
    "# === NORMALISATION DES NOMS MAROCAINS ===\n",
    "\n",
    "def normalize_name(name):\n",
    "    if not name or not isinstance(name, str):\n",
    "        return \"\"\n",
    "    name = name.lower()\n",
    "    # Variantes courantes dans les noms marocains/arabes\n",
    "    replacements = {\n",
    "        \"mohamed\": \"mohammed\",\n",
    "        \"mehdi\": \"mohammed\",\n",
    "        \"oussama\": \"usama\",\n",
    "        \"youssef\": \"yusuf\",\n",
    "        \"yassine\": \"yassin\",\n",
    "        \"el \": \"\",\n",
    "        \"al \": \"\",\n",
    "        \"ben \": \"\",\n",
    "        \"b \": \"\",\n",
    "        \"daoudi\": \"daoudy\",\n",
    "        \"ghizlani\": \"ghizlane\",\n",
    "        \"mhaimdat\": \"mhaimdane\",\n",
    "    }\n",
    "    for src, dst in replacements.items():\n",
    "        name = name.replace(src, dst)\n",
    "    # Nettoyage\n",
    "    name = re.sub(r\"[^a-z0-9\\s]\", \" \", name)\n",
    "    name = re.sub(r\"\\s+\", \" \", name).strip()\n",
    "    return name\n",
    "\n",
    "def extract_first_last(name):\n",
    "    if not name or not isinstance(name, str):\n",
    "        return \"\", \"\"\n",
    "    parts = name.strip().split()\n",
    "    if not parts:\n",
    "        return \"\", \"\"\n",
    "    first = parts[0].lower()\n",
    "    last = parts[-1].lower() if len(parts) > 1 else \"\"\n",
    "    return first, last\n",
    "\n",
    "def get_normalized_fullname(p):\n",
    "    return normalize_name(p.get(\"fullName\", \"\"))\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "\n",
    "THRESHOLDS = {\n",
    "    (\"github\", \"linkedin\"): 0.52,\n",
    "    (\"linkedin\", \"github\"): 0.52,\n",
    "    (\"github\", \"twitter\"): 0.48,\n",
    "    (\"twitter\", \"github\"): 0.48,\n",
    "    (\"linkedin\", \"twitter\"): 0.48,\n",
    "    (\"twitter\", \"linkedin\"): 0.48,\n",
    "}\n",
    "\n",
    "MOROCCAN_DOMAINS = {\n",
    "    \"1337.ma\", \"um5s.ac.ma\", \"ensam.ma\", \"emsi.ma\", \"um6p.ma\", \"uiz.ac.ma\",\n",
    "    \"gmail.com\", \"hotmail.com\", \"yahoo.fr\", \"protonmail.com\"  # inclus car tr√®s utilis√©s\n",
    "}\n",
    "\n",
    "MOROCCAN_CITIES = {\"casablanca\", \"rabat\", \"mohammedia\", \"marrakech\", \"fes\", \"agadir\", \"tanger\", \"meknes\", \"oujda\", \"kenitra\"}\n",
    "\n",
    "def is_moroccan_location(loc):\n",
    "    if not loc:\n",
    "        return False\n",
    "    loc = str(loc).lower()\n",
    "    return \"morocco\" in loc or \"maroc\" in loc or any(city in loc for city in MOROCCAN_CITIES)\n",
    "\n",
    "# === FONCTIONS DE FILTRAGE ===\n",
    "\n",
    "def get_dynamic_threshold(p1, p2):\n",
    "    key = (p1[\"platform\"], p2[\"platform\"])\n",
    "    base_thresh = THRESHOLDS.get(key, 0.50)\n",
    "\n",
    "    full_name_lex = jaro_winkler(get_normalized_fullname(p1), get_normalized_fullname(p2))\n",
    "    if full_name_lex >= 0.85:\n",
    "        return max(0.42, base_thresh - 0.08)\n",
    "\n",
    "    if (p1[\"email_norm\"] and p2[\"email_norm\"] and p1[\"email_norm\"] == p2[\"email_norm\"]) or \\\n",
    "       (p1[\"platform\"] == \"github\" and p2.get(\"username\") and p2[\"username\"].lower() in (p1.get(\"bio\", \"\") + \" \" + p1.get(\"repo_descriptions\", \"\")).lower()) or \\\n",
    "       (p2[\"platform\"] == \"github\" and p1.get(\"username\") and p1[\"username\"].lower() in (p2.get(\"bio\", \"\") + \" \" + p2.get(\"repo_descriptions\", \"\")).lower()):\n",
    "        return 0.38\n",
    "\n",
    "    return base_thresh\n",
    "\n",
    "def is_likely_false_positive(p1, p2, score):\n",
    "    name1_parts = p1.get(\"fullName\", \"\").strip().split()\n",
    "    name2_parts = p2.get(\"fullName\", \"\").strip().split()\n",
    "    if not name1_parts or not name2_parts:\n",
    "        return True\n",
    "\n",
    "    last1 = name1_parts[-1].lower()\n",
    "    last2 = name2_parts[-1].lower()\n",
    "    both_moroccan = is_moroccan_location(p1.get(\"location\")) and is_moroccan_location(p2.get(\"location\"))\n",
    "    same_first = p1[\"first\"] == p2[\"first\"]\n",
    "\n",
    "    # Cas marocain : tol√©rance accrue si pr√©nom identique\n",
    "    if both_moroccan and same_first and last1 != last2:\n",
    "        if score >= 0.48:\n",
    "            return False  # accept√©\n",
    "\n",
    "    # Cas g√©n√©ral : seuil strict\n",
    "    if last1 != last2 and score < 0.60:\n",
    "        gh_bio = \"\"\n",
    "        other_user = \"\"\n",
    "        if p1[\"platform\"] == \"github\":\n",
    "            gh_bio = (p1.get(\"bio\", \"\") + \" \" + p1.get(\"repo_descriptions\", \"\")).lower()\n",
    "            other_user = p2.get(\"username\", \"\").lower()\n",
    "        elif p2[\"platform\"] == \"github\":\n",
    "            gh_bio = (p2.get(\"bio\", \"\") + \" \" + p2.get(\"repo_descriptions\", \"\")).lower()\n",
    "            other_user = p1.get(\"username\", \"\").lower()\n",
    "        if not (other_user and other_user in gh_bio) and not (p1[\"email_norm\"] == p2[\"email_norm\"]):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def semantic_coherence(profiles, field_embeddings, i, j):\n",
    "    \"\"\"V√©rifie que les profils parlent du m√™me domaine technique.\"\"\"\n",
    "    fields = [\"bio\", \"repo_descriptions\", \"headline\"]\n",
    "    vecs1 = [field_embeddings[f][i] for f in fields if field_embeddings[f][i].any()]\n",
    "    vecs2 = [field_embeddings[f][j] for f in fields if field_embeddings[f][j].any()]\n",
    "    if not vecs1 or not vecs2:\n",
    "        return True\n",
    "    emb1 = np.mean(vecs1, axis=0)\n",
    "    emb2 = np.mean(vecs2, axis=0)\n",
    "    return cosine_sim(emb1, emb2) >= 0.38\n",
    "\n",
    "# === FONCTION PRINCIPALE ===\n",
    "\n",
    "def main():\n",
    "    print(\"üöÄ √âTAPE 3 : Matching Final Pond√©r√© ‚Äî CIBLE ‚â•1000 PAIRES SANS FAUX POSITIFS\")\n",
    "    print(\"   ‚Üí Normalisation des noms marocains\")\n",
    "    print(\"   ‚Üí Signaux locaux renforc√©s (.ma, villes, variantes)\")\n",
    "    print(\"   ‚Üí Coh√©rence s√©mantique obligatoire\")\n",
    "\n",
    "    output_dir = Path(\"output\")\n",
    "    with open(output_dir / \"profiles_metadata.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        profiles = json.load(f)\n",
    "    n = len(profiles)\n",
    "\n",
    "    # Charger embeddings\n",
    "    field_embeddings = {}\n",
    "    for field in [\"fullName\", \"username\", \"bio\", \"repo_descriptions\", \"headline\"]:\n",
    "        field_embeddings[field] = np.load(output_dir / f\"{field}_embeddings.npy\")\n",
    "\n",
    "    # Enrichir profils\n",
    "    for p in profiles:\n",
    "        p[\"email_norm\"] = normalize_email(p.get(\"email\"))\n",
    "        p[\"email_domain\"] = get_email_domain(p.get(\"email\"))\n",
    "        p[\"first\"], p[\"last\"] = extract_first_last(p.get(\"fullName\", \"\"))\n",
    "        p[\"norm_fullname\"] = get_normalized_fullname(p)\n",
    "\n",
    "    # Indexation avanc√©e\n",
    "    email_to_idx = defaultdict(list)\n",
    "    first_name_to_idx = defaultdict(list)\n",
    "    init_city_to_idx = defaultdict(list)\n",
    "    username_prefix_to_idx = defaultdict(list)\n",
    "    blocking_key_to_idx = defaultdict(list)\n",
    "\n",
    "    for i, p in enumerate(profiles):\n",
    "        loc = p.get(\"location\")\n",
    "        is_moroccan = is_moroccan_location(loc)\n",
    "\n",
    "        if p[\"email_norm\"]:\n",
    "            email_to_idx[p[\"email_norm\"]].append(i)\n",
    "        if p[\"first\"]:\n",
    "            first_name_to_idx[p[\"first\"]].append(i)\n",
    "            if is_moroccan:\n",
    "                init_city_to_idx[(p[\"first\"][0], \"morocco\")].append(i)\n",
    "        key = (p[\"first\"][:2] if p[\"first\"] else \"\", p[\"last\"][0] if p[\"last\"] else \"\")\n",
    "        blocking_key_to_idx[key].append(i)\n",
    "        if p.get(\"username\"):\n",
    "            u = p[\"username\"].lower()\n",
    "            if len(u) >= 4:\n",
    "                username_prefix_to_idx[u[:4]].append(i)\n",
    "\n",
    "    def compute_score(i, j):\n",
    "        p1, p2 = profiles[i], profiles[j]\n",
    "        platform1, platform2 = p1[\"platform\"], p2[\"platform\"]\n",
    "        score = 0.0\n",
    "\n",
    "        # 1. Email exact\n",
    "        if p1[\"email_norm\"] and p2[\"email_norm\"] and p1[\"email_norm\"] == p2[\"email_norm\"]:\n",
    "            score += 0.30\n",
    "\n",
    "        # 2. Liens GitHub ‚Üí autre plateforme\n",
    "        gh = p1 if p1[\"platform\"] == \"github\" else (p2 if p2[\"platform\"] == \"github\" else None)\n",
    "        other = p2 if gh is p1 else (p1 if gh is p2 else None)\n",
    "        if gh and other:\n",
    "            gh_text = (gh.get(\"bio\", \"\") + \" \" + gh.get(\"repo_descriptions\", \"\")).lower()\n",
    "            other_user = other.get(\"username\", \"\").lower()\n",
    "            if other_user and other_user in gh_text:\n",
    "                score += 0.30\n",
    "\n",
    "        # 3. fullName (normalis√© via embeddings)\n",
    "        score += 0.25 * cosine_sim(field_embeddings[\"fullName\"][i], field_embeddings[\"fullName\"][j])\n",
    "\n",
    "        # 4. username\n",
    "        score += 0.10 * cosine_sim(field_embeddings[\"username\"][i], field_embeddings[\"username\"][j])\n",
    "\n",
    "        # 5. Bonus: username identique (GitHub ‚Üî Twitter)\n",
    "        if {p1[\"platform\"], p2[\"platform\"]} == {\"github\", \"twitter\"}:\n",
    "            u1 = p1.get(\"username\", \"\").lower()\n",
    "            u2 = p2.get(\"username\", \"\").lower()\n",
    "            if u1 and u2 and u1 == u2:\n",
    "                score += 0.20\n",
    "\n",
    "        # 6. bio\n",
    "        score += 0.05 * cosine_sim(field_embeddings[\"bio\"][i], field_embeddings[\"bio\"][j])\n",
    "\n",
    "        # 7. repo ‚Üî headline\n",
    "        if (platform1 == \"github\" and platform2 == \"linkedin\"):\n",
    "            score += 0.30 * cosine_sim(field_embeddings[\"repo_descriptions\"][i], field_embeddings[\"headline\"][j])\n",
    "        elif (platform1 == \"linkedin\" and platform2 == \"github\"):\n",
    "            score += 0.30 * cosine_sim(field_embeddings[\"repo_descriptions\"][j], field_embeddings[\"headline\"][i])\n",
    "\n",
    "        # 8. Bonus: domaine email marocain\n",
    "        if p1[\"email_domain\"] and p2[\"email_domain\"] and p1[\"email_domain\"] == p2[\"email_domain\"]:\n",
    "            if any(dom in p1[\"email_domain\"] for dom in [\"1337.ma\", \"um5s.ac.ma\", \"ensam.ma\", \"emsi.ma\", \"um6p.ma\"]):\n",
    "                score += 0.15\n",
    "\n",
    "        # 9. Bonus: localisation Maroc + pr√©nom commun + similarit√© nom ‚â•0.65\n",
    "        if p1[\"first\"] == p2[\"first\"] and is_moroccan_location(p1.get(\"location\")) and is_moroccan_location(p2.get(\"location\")):\n",
    "            last_sim = jaro_winkler(p1[\"norm_fullname\"], p2[\"norm_fullname\"])\n",
    "            if last_sim >= 0.65:\n",
    "                score += 0.10\n",
    "\n",
    "        return min(1.0, score)\n",
    "\n",
    "    candidate_pairs = []\n",
    "\n",
    "    for i in range(n):\n",
    "        p = profiles[i]\n",
    "        candidates = set()\n",
    "\n",
    "        if p[\"email_norm\"]:\n",
    "            candidates.update(email_to_idx[p[\"email_norm\"]])\n",
    "        if p[\"first\"]:\n",
    "            candidates.update(first_name_to_idx[p[\"first\"]])\n",
    "            candidates.update(init_city_to_idx.get((p[\"first\"][0], \"morocco\"), []))\n",
    "        key = (p[\"first\"][:2] if p[\"first\"] else \"\", p[\"last\"][0] if p[\"last\"] else \"\")\n",
    "        candidates.update(blocking_key_to_idx[key])\n",
    "        if p.get(\"username\"):\n",
    "            u = p[\"username\"].lower()\n",
    "            if len(u) >= 4:\n",
    "                candidates.update(username_prefix_to_idx.get(u[:4], []))\n",
    "        if p[\"platform\"] == \"github\":\n",
    "            gh_text = (p.get(\"bio\", \"\") + \" \" + p.get(\"repo_descriptions\", \"\")).lower()\n",
    "            for j in range(n):\n",
    "                if i == j or profiles[j][\"platform\"] == \"github\":\n",
    "                    continue\n",
    "                other_user = profiles[j].get(\"username\", \"\").lower()\n",
    "                if other_user and other_user in gh_text:\n",
    "                    candidates.add(j)\n",
    "\n",
    "        for j in candidates:\n",
    "            if i >= j or profiles[i][\"platform\"] == profiles[j][\"platform\"]:\n",
    "                continue\n",
    "\n",
    "            p1, p2 = profiles[i], profiles[j]\n",
    "\n",
    "            # Tol√©rance sur pr√©nom si email identique\n",
    "            if p1[\"first\"] != p2[\"first\"]:\n",
    "                if not (p1[\"email_norm\"] and p2[\"email_norm\"] and p1[\"email_norm\"] == p2[\"email_norm\"]):\n",
    "                    continue\n",
    "\n",
    "            # Signaux de confiance obligatoires\n",
    "            full_name_cos = cosine_sim(field_embeddings[\"fullName\"][i], field_embeddings[\"fullName\"][j])\n",
    "            full_name_lex = jaro_winkler(p1[\"norm_fullname\"], p2[\"norm_fullname\"])\n",
    "\n",
    "            has_trusted_signal = (\n",
    "                (p1[\"email_norm\"] and p2[\"email_norm\"] and p1[\"email_norm\"] == p2[\"email_norm\"]) or\n",
    "                (p1[\"platform\"] == \"github\" and p2.get(\"username\") and p2[\"username\"].lower() in (p1.get(\"bio\", \"\") + \" \" + p1.get(\"repo_descriptions\", \"\")).lower()) or\n",
    "                (p2[\"platform\"] == \"github\" and p1.get(\"username\") and p1[\"username\"].lower() in (p2.get(\"bio\", \"\") + \" \" + p2.get(\"repo_descriptions\", \"\")).lower()) or\n",
    "                (full_name_lex >= 0.78) or\n",
    "                (full_name_cos >= 0.72)\n",
    "            )\n",
    "            if not has_trusted_signal:\n",
    "                continue\n",
    "\n",
    "            if not semantic_coherence(profiles, field_embeddings, i, j):\n",
    "                continue\n",
    "\n",
    "            score = compute_score(i, j)\n",
    "            dynamic_thresh = get_dynamic_threshold(p1, p2)\n",
    "            if score >= dynamic_thresh:\n",
    "                if not is_likely_false_positive(p1, p2, score):\n",
    "                    candidate_pairs.append((i, j, score))\n",
    "\n",
    "    # Matching 1:1 strict\n",
    "    best_match = {}\n",
    "    for i, j, score in candidate_pairs:\n",
    "        if i not in best_match or best_match[i][1] < score:\n",
    "            best_match[i] = (j, score)\n",
    "        if j not in best_match or best_match[j][1] < score:\n",
    "            best_match[j] = (i, score)\n",
    "\n",
    "    final_matches = []\n",
    "    used = set()\n",
    "    for i, j, score in candidate_pairs:\n",
    "        if i in used or j in used:\n",
    "            continue\n",
    "        if best_match.get(i) == (j, score) and best_match.get(j) == (i, score):\n",
    "            final_matches.append((i, j, score))\n",
    "            used.add(i)\n",
    "            used.add(j)\n",
    "\n",
    "    # Transitivit√© (Union-Find)\n",
    "    parent = list(range(n))\n",
    "    def find(x):\n",
    "        if parent[x] != x:\n",
    "            parent[x] = find(parent[x])\n",
    "        return parent[x]\n",
    "    def union(x, y):\n",
    "        rx, ry = find(x), find(y)\n",
    "        if rx != ry:\n",
    "            parent[ry] = rx\n",
    "\n",
    "    for i, j, _ in final_matches:\n",
    "        union(i, j)\n",
    "\n",
    "    components = defaultdict(list)\n",
    "    for i in range(n):\n",
    "        root = find(i)\n",
    "        components[root].append(i)\n",
    "\n",
    "    unified = []\n",
    "    for comp in components.values():\n",
    "        unified.append({\n",
    "            \"unified_id\": f\"person_{len(unified):05d}\",\n",
    "            \"profiles\": [profiles[i] for i in comp]\n",
    "        })\n",
    "\n",
    "    with open(output_dir / \"unified_profiles.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(unified, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"\\n‚úÖ {len(final_matches)} paires valides trouv√©es\")\n",
    "    print(f\"‚úÖ {len(unified)} identit√©s unifi√©es (sur {n} profils initiaux)\")\n",
    "    print(\"‚úÖ R√©sultats sauvegard√©s dans 'output/unified_profiles.json'\")\n",
    "\n",
    "    print(\"\\nüîç Top 20 paires trouv√©es :\")\n",
    "    for idx, (i, j, score) in enumerate(final_matches[:20], 1):\n",
    "        p1, p2 = profiles[i], profiles[j]\n",
    "        print(f\"{idx:2d}. [{score:.3f}] \"\n",
    "              f\"{p1.get('fullName', 'N/A')} ({p1['platform']}) ‚Üî \"\n",
    "              f\"{p2.get('fullName', 'N/A')} ({p2['platform']}) | \"\n",
    "              f\"Loc: {p1.get('location', 'N/A')} / {p2.get('location', 'N/A')}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3718d51b-3319-40e3-abac-2a78f0f599fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ √âTAPE 3 : Matching Final Pond√©r√© ‚Äî OPTIMIS√â POUR ‚â•100 PAIRES\n",
      "   ‚Üí Ciblage des profils tech marocains (ex: Omar MHAIMDAT)\n",
      "   ‚Üí √âquilibre rappel/pr√©cision\n",
      "\n",
      "‚úÖ 109 paires valides trouv√©es\n",
      "‚úÖ 11290 identit√©s unifi√©es (sur 11399 profils initiaux)\n",
      "‚úÖ R√©sultats sauvegard√©s dans 'output/unified_profiles.json'\n",
      "\n",
      "üîç Top 20 paires trouv√©es :\n",
      " 1. [0.528] IKRAM (github) ‚Üî Ikram Daoudi (linkedin) | Loc: morocco / other\n",
      " 2. [0.762] Taibi EL Yakouti (github) ‚Üî Taibi El Yakouti (linkedin) | Loc: morocco / other\n",
      " 3. [0.547] Lamiae Hana (github) ‚Üî Lamiae Hana (linkedin) | Loc: morocco / other\n",
      " 4. [0.518] Ayoub Najjout (github) ‚Üî Ayoub Najjout (linkedin) | Loc: morocco / other\n",
      " 5. [0.505] Boutaina ELYAZIJI (github) ‚Üî Boutaina ELYAZIJI (linkedin) | Loc: morocco / other\n",
      " 6. [0.502] Abdelmoughit ASSAL (github) ‚Üî Abdelmoughit Assal (linkedin) | Loc: morocco / other\n",
      " 7. [0.671] Yassir Acharki (github) ‚Üî Yassir Acharki (twitter) | Loc: morocco / other\n",
      " 8. [0.599] Brahim Alaoui (github) ‚Üî Brahim AADIL (linkedin) | Loc: morocco / morocco\n",
      " 9. [0.663] Khadija Mouhtaj (github) ‚Üî Khadija Mekouar (linkedin) | Loc: morocco / other\n",
      "10. [0.530] Bouarfa Lahmar (github) ‚Üî Bouarfa Lahmar (linkedin) | Loc: morocco / other\n",
      "11. [0.525] Mohammed Nabil (github) ‚Üî Mohammed Nabil (linkedin) | Loc: morocco / other\n",
      "12. [0.501] Hicham Nouhaidi (github) ‚Üî Hicham Nouhaidi (linkedin) | Loc: morocco / other\n",
      "13. [0.506] Hamza Limouri (github) ‚Üî Hamza LIMOURI (linkedin) | Loc: morocco / other\n",
      "14. [0.528] Aimad SADOUK (github) ‚Üî Aimad SADOUK (linkedin) | Loc: morocco / other\n",
      "15. [0.610] Oussama RAJI (github) ‚Üî Oussama K (linkedin) | Loc: morocco / other\n",
      "16. [0.538] Ismail Tarik (github) ‚Üî ISMAIL LARHCHIM (linkedin) | Loc: morocco / other\n",
      "17. [0.560] Younes Bousetta (github) ‚Üî Younes Bousetta (linkedin) | Loc: morocco / other\n",
      "18. [0.508] Naouar EL BOUMASHOULI (github) ‚Üî Naouar EL BOUMASHOULI (linkedin) | Loc: morocco / other\n",
      "19. [0.519] Aicha ZEROUAL (github) ‚Üî Aicha El oubaydi (linkedin) | Loc: morocco / other\n",
      "20. [0.512] Mohamed Aqlil (github) ‚Üî Mohamed Aqlil (linkedin) | Loc: morocco / other\n"
     ]
    }
   ],
   "source": [
    "# step3_weighted_matching.py\n",
    "\"\"\"\n",
    "√âTAPE 3 : Matching Final Pond√©r√© ‚Äî VERSION OPTIMIS√âE\n",
    "- Cible ‚â•100 paires fiables\n",
    "- √âvite les faux positifs\n",
    "- Supporte les profils techniques marocains (ex: Omar MHAIMDAT)\n",
    "- Compatible avec step2_semantic_representation.py\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "def normalize_email(email):\n",
    "    if not email or \"@\" not in str(email):\n",
    "        return None\n",
    "    return str(email).strip().lower()\n",
    "\n",
    "def cosine_sim(u, v):\n",
    "    norm_u = np.linalg.norm(u)\n",
    "    norm_v = np.linalg.norm(v)\n",
    "    if norm_u == 0 or norm_v == 0:\n",
    "        return 0.0\n",
    "    return np.dot(u, v) / (norm_u * norm_v + 1e-8)\n",
    "\n",
    "def jaro_winkler(s1, s2):\n",
    "    if not s1 or not s2:\n",
    "        return 0.0\n",
    "    s1, s2 = s1.lower(), s2.lower()\n",
    "    if s1 == s2:\n",
    "        return 1.0\n",
    "    len1, len2 = len(s1), len(s2)\n",
    "    match_window = max(len1, len2) // 2 - 1\n",
    "    match_window = max(0, match_window)\n",
    "\n",
    "    s1_matches = [False] * len1\n",
    "    s2_matches = [False] * len2\n",
    "    matches = 0\n",
    "    transpositions = 0\n",
    "\n",
    "    for i in range(len1):\n",
    "        start = max(0, i - match_window)\n",
    "        end = min(i + match_window + 1, len2)\n",
    "        for j in range(start, end):\n",
    "            if s2_matches[j] or s1[i] != s2[j]:\n",
    "                continue\n",
    "            s1_matches[i] = s2_matches[j] = True\n",
    "            matches += 1\n",
    "            break\n",
    "\n",
    "    if matches == 0:\n",
    "        return 0.0\n",
    "\n",
    "    k = 0\n",
    "    for i in range(len1):\n",
    "        if not s1_matches[i]:\n",
    "            continue\n",
    "        while not s2_matches[k]:\n",
    "            k += 1\n",
    "        if s1[i] != s2[k]:\n",
    "            transpositions += 1\n",
    "        k += 1\n",
    "\n",
    "    jaro = (matches/len1 + matches/len2 + (matches - transpositions/2)/matches) / 3.0\n",
    "    prefix = 0\n",
    "    for i in range(min(4, min(len1, len2))):\n",
    "        if s1[i] == s2[i]:\n",
    "            prefix += 1\n",
    "        else:\n",
    "            break\n",
    "    return jaro + (prefix * 0.1 * (1 - jaro))\n",
    "\n",
    "def extract_first_last(name):\n",
    "    if not name or not isinstance(name, str):\n",
    "        return \"\", \"\"\n",
    "    parts = name.strip().split()\n",
    "    if not parts:\n",
    "        return \"\", \"\"\n",
    "    first = parts[0].lower()\n",
    "    last = parts[-1].lower() if len(parts) > 1 else \"\"\n",
    "    return first, last\n",
    "\n",
    "# -----------------------------\n",
    "# Seuils de base\n",
    "# -----------------------------\n",
    "THRESHOLDS = {\n",
    "    (\"github\", \"linkedin\"): 0.55,\n",
    "    (\"linkedin\", \"github\"): 0.55,\n",
    "    (\"github\", \"twitter\"): 0.50,\n",
    "    (\"twitter\", \"github\"): 0.50,\n",
    "    (\"linkedin\", \"twitter\"): 0.50,\n",
    "    (\"twitter\", \"linkedin\"): 0.50,\n",
    "}\n",
    "\n",
    "def get_dynamic_threshold(p1, p2):\n",
    "    key = (p1[\"platform\"], p2[\"platform\"])\n",
    "    base_thresh = THRESHOLDS.get(key, 0.50)\n",
    "\n",
    "    full_name_lex = jaro_winkler(p1.get(\"fullName\", \"\"), p2.get(\"fullName\", \"\"))\n",
    "    if full_name_lex >= 0.85:\n",
    "        return max(0.45, base_thresh - 0.05)\n",
    "\n",
    "    # Liens explicites ou email identique ‚Üí seuil tr√®s bas\n",
    "    if (p1[\"email_norm\"] and p2[\"email_norm\"] and p1[\"email_norm\"] == p2[\"email_norm\"]) or \\\n",
    "       (p1[\"platform\"] == \"github\" and p2.get(\"username\") and p2[\"username\"].lower() in (p1.get(\"bio\", \"\") + \" \" + p1.get(\"repo_descriptions\", \"\")).lower()) or \\\n",
    "       (p2[\"platform\"] == \"github\" and p1.get(\"username\") and p1[\"username\"].lower() in (p2.get(\"bio\", \"\") + \" \" + p2.get(\"repo_descriptions\", \"\")).lower()):\n",
    "        return 0.40\n",
    "\n",
    "    return base_thresh\n",
    "\n",
    "def is_likely_false_positive(p1, p2, score):\n",
    "    name1_parts = p1.get(\"fullName\", \"\").strip().split()\n",
    "    name2_parts = p2.get(\"fullName\", \"\").strip().split()\n",
    "    if not name1_parts or not name2_parts:\n",
    "        return True\n",
    "\n",
    "    last1 = name1_parts[-1].lower()\n",
    "    last2 = name2_parts[-1].lower()\n",
    "\n",
    "    # Si noms de famille tr√®s diff√©rents ET pas de lien fort\n",
    "    if last1 != last2 and score < 0.60:\n",
    "        gh_bio = \"\"\n",
    "        other_user = \"\"\n",
    "        if p1[\"platform\"] == \"github\":\n",
    "            gh_bio = (p1.get(\"bio\", \"\") + \" \" + p1.get(\"repo_descriptions\", \"\")).lower()\n",
    "            other_user = p2.get(\"username\", \"\").lower()\n",
    "        elif p2[\"platform\"] == \"github\":\n",
    "            gh_bio = (p2.get(\"bio\", \"\") + \" \" + p2.get(\"repo_descriptions\", \"\")).lower()\n",
    "            other_user = p1.get(\"username\", \"\").lower()\n",
    "        # Pas de lien explicite ni email commun ?\n",
    "        if not (other_user and other_user in gh_bio) and not (p1[\"email_norm\"] == p2[\"email_norm\"]):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def main():\n",
    "    print(\"üöÄ √âTAPE 3 : Matching Final Pond√©r√© ‚Äî OPTIMIS√â POUR ‚â•100 PAIRES\")\n",
    "    print(\"   ‚Üí Ciblage des profils tech marocains (ex: Omar MHAIMDAT)\")\n",
    "    print(\"   ‚Üí √âquilibre rappel/pr√©cision\")\n",
    "\n",
    "    output_dir = Path(\"output\")\n",
    "    with open(output_dir / \"profiles_metadata.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        profiles = json.load(f)\n",
    "    n = len(profiles)\n",
    "\n",
    "    # Charger embeddings\n",
    "    field_embeddings = {}\n",
    "    for field in [\"fullName\", \"username\", \"bio\", \"repo_descriptions\", \"headline\"]:\n",
    "        field_embeddings[field] = np.load(output_dir / f\"{field}_embeddings.npy\")\n",
    "\n",
    "    # Enrichir profils\n",
    "    for p in profiles:\n",
    "        p[\"email_norm\"] = normalize_email(p.get(\"email\"))\n",
    "        p[\"first\"], p[\"last\"] = extract_first_last(p.get(\"fullName\", \"\"))\n",
    "\n",
    "    # Indexation avanc√©e\n",
    "    email_to_idx = defaultdict(list)\n",
    "    first_name_to_idx = defaultdict(list)\n",
    "    blocking_key_to_idx = defaultdict(list)\n",
    "    username_to_idx = defaultdict(list)\n",
    "    location_to_idx = defaultdict(list)  # ‚Üê ajout pour profils marocains\n",
    "\n",
    "    moroccan_cities = {\"casablanca\", \"rabat\", \"mohammedia\", \"marrakech\", \"fes\", \"agadir\", \"tanger\", \"meknes\", \"oujda\", \"kenitra\"}\n",
    "\n",
    "    for i, p in enumerate(profiles):\n",
    "        loc = str(p.get(\"location\", \"\")).lower()\n",
    "        is_moroccan = \"morocco\" in loc or \"maroc\" in loc or any(city in loc for city in moroccan_cities)\n",
    "\n",
    "        if p[\"email_norm\"]:\n",
    "            email_to_idx[p[\"email_norm\"]].append(i)\n",
    "        if p[\"first\"]:\n",
    "            first_name_to_idx[p[\"first\"]].append(i)\n",
    "            if is_moroccan:\n",
    "                location_to_idx[(p[\"first\"], \"morocco\")].append(i)\n",
    "        key = (p[\"first\"][:2], p[\"last\"][0] if p[\"last\"] else \"\")\n",
    "        blocking_key_to_idx[key].append(i)\n",
    "        if p.get(\"username\"):\n",
    "            username_to_idx[p[\"username\"].lower()].append(i)\n",
    "\n",
    "    def compute_score(i, j):\n",
    "        p1, p2 = profiles[i], profiles[j]\n",
    "        platform1, platform2 = p1[\"platform\"], p2[\"platform\"]\n",
    "        score = 0.0\n",
    "\n",
    "        # 1. Email exact ‚Üí 0.30\n",
    "        if p1[\"email_norm\"] and p2[\"email_norm\"] and p1[\"email_norm\"] == p2[\"email_norm\"]:\n",
    "            score += 0.30\n",
    "\n",
    "        # 2. Liens externes (GitHub bio contient username)\n",
    "        gh = p1 if p1[\"platform\"] == \"github\" else (p2 if p2[\"platform\"] == \"github\" else None)\n",
    "        other = p2 if gh is p1 else (p1 if gh is p2 else None)\n",
    "        if gh and other:\n",
    "            gh_text = (gh.get(\"bio\", \"\") + \" \" + gh.get(\"repo_descriptions\", \"\")).lower()\n",
    "            other_user = other.get(\"username\", \"\").lower()\n",
    "            if other_user and other_user in gh_text:\n",
    "                score += 0.30\n",
    "\n",
    "        # 3. fullName ‚Üí 0.25\n",
    "        score += 0.25 * cosine_sim(field_embeddings[\"fullName\"][i], field_embeddings[\"fullName\"][j])\n",
    "\n",
    "        # 4. username ‚Üí 0.10\n",
    "        score += 0.10 * cosine_sim(field_embeddings[\"username\"][i], field_embeddings[\"username\"][j])\n",
    "\n",
    "        # 5. bio ‚Üí 0.05\n",
    "        score += 0.05 * cosine_sim(field_embeddings[\"bio\"][i], field_embeddings[\"bio\"][j])\n",
    "\n",
    "        # 6. repo ‚Üî headline (GitHub ‚Üî LinkedIn)\n",
    "        if (platform1 == \"github\" and platform2 == \"linkedin\"):\n",
    "            score += 0.30 * cosine_sim(field_embeddings[\"repo_descriptions\"][i], field_embeddings[\"headline\"][j])\n",
    "        elif (platform1 == \"linkedin\" and platform2 == \"github\"):\n",
    "            score += 0.30 * cosine_sim(field_embeddings[\"repo_descriptions\"][j], field_embeddings[\"headline\"][i])\n",
    "\n",
    "        return min(1.0, score)\n",
    "\n",
    "    candidate_pairs = []\n",
    "\n",
    "    for i in range(n):\n",
    "        p = profiles[i]\n",
    "        candidates = set()\n",
    "\n",
    "        if p[\"email_norm\"]:\n",
    "            candidates.update(email_to_idx[p[\"email_norm\"]])\n",
    "        if p[\"first\"]:\n",
    "            candidates.update(first_name_to_idx[p[\"first\"]])\n",
    "            # Ajout cibl√© pour profils marocains\n",
    "            candidates.update(location_to_idx.get((p[\"first\"], \"morocco\"), []))\n",
    "        key = (p[\"first\"][:2], p[\"last\"][0] if p[\"last\"] else \"\")\n",
    "        candidates.update(blocking_key_to_idx[key])\n",
    "        if p[\"platform\"] == \"github\":\n",
    "            gh_text = (p.get(\"bio\", \"\") + \" \" + p.get(\"repo_descriptions\", \"\")).lower()\n",
    "            for j in range(n):\n",
    "                if i == j or profiles[j][\"platform\"] == \"github\":\n",
    "                    continue\n",
    "                other_user = profiles[j].get(\"username\", \"\").lower()\n",
    "                if other_user and other_user in gh_text:\n",
    "                    candidates.add(j)\n",
    "\n",
    "        for j in candidates:\n",
    "            if i >= j or profiles[i][\"platform\"] == profiles[j][\"platform\"]:\n",
    "                continue\n",
    "\n",
    "            p1, p2 = profiles[i], profiles[j]\n",
    "            if p1[\"first\"] != p2[\"first\"]:\n",
    "                continue\n",
    "\n",
    "            # Signaux de confiance obligatoires\n",
    "            full_name_cos = cosine_sim(field_embeddings[\"fullName\"][i], field_embeddings[\"fullName\"][j])\n",
    "            full_name_lex = jaro_winkler(p1.get(\"fullName\", \"\"), p2.get(\"fullName\", \"\"))\n",
    "\n",
    "            has_trusted_signal = (\n",
    "                (p1[\"email_norm\"] and p2[\"email_norm\"] and p1[\"email_norm\"] == p2[\"email_norm\"]) or\n",
    "                (p1[\"platform\"] == \"github\" and p2.get(\"username\") and p2[\"username\"].lower() in (p1.get(\"bio\", \"\") + \" \" + p1.get(\"repo_descriptions\", \"\")).lower()) or\n",
    "                (p2[\"platform\"] == \"github\" and p1.get(\"username\") and p1[\"username\"].lower() in (p2.get(\"bio\", \"\") + \" \" + p2.get(\"repo_descriptions\", \"\")).lower()) or\n",
    "                (full_name_lex >= 0.80) or\n",
    "                (full_name_cos >= 0.75)\n",
    "            )\n",
    "            if not has_trusted_signal:\n",
    "                continue\n",
    "\n",
    "            score = compute_score(i, j)\n",
    "            dynamic_thresh = get_dynamic_threshold(p1, p2)\n",
    "            if score >= dynamic_thresh:\n",
    "                if not is_likely_false_positive(p1, p2, score):\n",
    "                    candidate_pairs.append((i, j, score))\n",
    "\n",
    "    # Matching 1:1 strict\n",
    "    best_match = {}\n",
    "    for i, j, score in candidate_pairs:\n",
    "        if i not in best_match or best_match[i][1] < score:\n",
    "            best_match[i] = (j, score)\n",
    "        if j not in best_match or best_match[j][1] < score:\n",
    "            best_match[j] = (i, score)\n",
    "\n",
    "    final_matches = []\n",
    "    used = set()\n",
    "    for i, j, score in candidate_pairs:\n",
    "        if i in used or j in used:\n",
    "            continue\n",
    "        if best_match.get(i) == (j, score) and best_match.get(j) == (i, score):\n",
    "            final_matches.append((i, j, score))\n",
    "            used.add(i)\n",
    "            used.add(j)\n",
    "\n",
    "    # Transitivit√© (Union-Find)\n",
    "    parent = list(range(n))\n",
    "    def find(x):\n",
    "        if parent[x] != x:\n",
    "            parent[x] = find(parent[x])\n",
    "        return parent[x]\n",
    "    def union(x, y):\n",
    "        rx, ry = find(x), find(y)\n",
    "        if rx != ry:\n",
    "            parent[ry] = rx\n",
    "\n",
    "    for i, j, _ in final_matches:\n",
    "        union(i, j)\n",
    "\n",
    "    # Clusters unifi√©s\n",
    "    components = defaultdict(list)\n",
    "    for i in range(n):\n",
    "        root = find(i)\n",
    "        components[root].append(i)\n",
    "\n",
    "    unified = []\n",
    "    for comp in components.values():\n",
    "        unified.append({\n",
    "            \"unified_id\": f\"person_{len(unified):05d}\",\n",
    "            \"profiles\": [profiles[i] for i in comp]\n",
    "        })\n",
    "\n",
    "    with open(output_dir / \"unified_profiles.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(unified, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"\\n‚úÖ {len(final_matches)} paires valides trouv√©es\")\n",
    "    print(f\"‚úÖ {len(unified)} identit√©s unifi√©es (sur {n} profils initiaux)\")\n",
    "    print(\"‚úÖ R√©sultats sauvegard√©s dans 'output/unified_profiles.json'\")\n",
    "\n",
    "    # üîç Affichage des matches\n",
    "    print(\"\\nüîç Top 20 paires trouv√©es :\")\n",
    "    for idx, (i, j, score) in enumerate(final_matches[:20], 1):\n",
    "        p1, p2 = profiles[i], profiles[j]\n",
    "        print(f\"{idx:2d}. [{score:.3f}] \"\n",
    "              f\"{p1.get('fullName', 'N/A')} ({p1['platform']}) ‚Üî \"\n",
    "              f\"{p2.get('fullName', 'N/A')} ({p2['platform']}) | \"\n",
    "              f\"Loc: {p1.get('location', '')} / {p2.get('location', '')}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6772b74-0a8c-4537-a62d-fb510dacc4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ √âTAPE 3 : Matching Final Pond√©r√© ‚Äî VERSION RENFORC√âE\n",
      "   ‚Üí Cible ‚â•150 paires fiables\n",
      "   ‚Üí Seuils √©lev√©s + poids augment√©s\n",
      "   ‚Üí Optimis√© pour profils tech marocains (ex: Omar MHAIMDAT)\n",
      "\n",
      "‚úÖ 265 paires valides trouv√©es (cible ‚â•150)\n",
      "‚úÖ 11134 identit√©s unifi√©es (sur 11399 profils initiaux)\n",
      "‚úÖ R√©sultats sauvegard√©s dans 'output/unified_profiles.json'\n",
      "\n",
      "üîç Top 20 paires trouv√©es :\n",
      " 1. [0.928] Taibi EL Yakouti (github) ‚Üî Taibi El Yakouti (linkedin) | Loc: morocco / other\n",
      " 2. [0.918] Zhengfa Tang (github) ‚Üî Zhengfa Tang (linkedin) | Loc: morocco / other\n",
      " 3. [0.909] Ajrass Tajemouti (github) ‚Üî Ajrass Tajemouti (linkedin) | Loc: morocco / other\n",
      " 4. [0.904] Zakaria El bouzkri (github) ‚Üî Zakaria El Bouzkri (linkedin) | Loc: morocco / other\n",
      " 5. [0.903] Younes M (github) ‚Üî Younes Mazouz (linkedin) | Loc: morocco / other\n",
      " 6. [0.832] Amal Senhaji (github) ‚Üî Amal Azimova PMP (linkedin) | Loc: morocco / other\n",
      " 7. [0.823] zakaria chahboun (github) ‚Üî zakaria chahboun (twitter) | Loc: morocco / other\n",
      " 8. [0.821] Yassir Acharki (github) ‚Üî Yassir Acharki (twitter) | Loc: morocco / other\n",
      " 9. [0.820] Youssef Abidi (github) ‚Üî Youssef Abed (linkedin) | Loc: morocco / other\n",
      "10. [0.817] Khadija Mouhtaj (github) ‚Üî Khadija Mekouar (linkedin) | Loc: morocco / other\n",
      "11. [0.810] HAMZA ROUGANI (github) ‚Üî Hamza M (linkedin) | Loc: morocco / other\n",
      "12. [0.806] Mohammed ali cheddad (github) ‚Üî Mohammed Azhar Pathan (linkedin) | Loc: morocco / other\n",
      "13. [0.805] Nouhaila Madrani (github) ‚Üî Nouhaila Salah El Kheir (linkedin) | Loc: morocco / other\n",
      "14. [0.801] Zayd inani (github) ‚Üî zayd inani (twitter) | Loc: other / other\n",
      "15. [0.795] Othmane (github) ‚Üî Othmane (twitter) | Loc: morocco / other\n",
      "16. [0.788] Reda DAALABI (github) ‚Üî Reda DAALABI (twitter) | Loc: morocco / other\n",
      "17. [0.787] Jayesh Lalwani (github) ‚Üî Jayesh Lalwani (twitter) | Loc: morocco / other\n",
      "18. [0.785] Yassine YASSIF (github) ‚Üî Yassine (twitter) | Loc: morocco / other\n",
      "19. [0.781] Youness Boualam (github) ‚Üî Youness BOUFELOUSSEN (linkedin) | Loc: morocco / other\n",
      "20. [0.779] Mouad Benali (github) ‚Üî Mouad Benali (twitter) | Loc: morocco / other\n"
     ]
    }
   ],
   "source": [
    "# step3_weighted_matching.py ‚Äî VERSION AM√âLIOR√âE\n",
    "\"\"\"\n",
    "√âTAPE 3 : Matching Final Pond√©r√© ‚Äî VERSION OPTIMIS√âE & RENFORC√âE\n",
    "- Cible ‚â•150 paires fiables (vs ‚â•100)\n",
    "- Seuils de similarit√© plus √©lev√©s (‚â•0.60 par d√©faut)\n",
    "- Poids augment√©s pour signaux fiables\n",
    "- Techniques de similarit√© avanc√©es (Jaro-Winkler + embeddings pond√©r√©s)\n",
    "- Support renforc√© pour profils techniques marocains (ex: Omar MHAIMDAT)\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def normalize_email(email):\n",
    "    if not email or \"@\" not in str(email):\n",
    "        return None\n",
    "    return str(email).strip().lower()\n",
    "\n",
    "def cosine_sim(u, v):\n",
    "    norm_u = np.linalg.norm(u)\n",
    "    norm_v = np.linalg.norm(v)\n",
    "    if norm_u == 0 or norm_v == 0:\n",
    "        return 0.0\n",
    "    return np.dot(u, v) / (norm_u * norm_v + 1e-8)\n",
    "\n",
    "def jaro_winkler(s1, s2):\n",
    "    if not s1 or not s2:\n",
    "        return 0.0\n",
    "    s1, s2 = s1.lower(), s2.lower()\n",
    "    if s1 == s2:\n",
    "        return 1.0\n",
    "    len1, len2 = len(s1), len(s2)\n",
    "    match_window = max(len1, len2) // 2 - 1\n",
    "    match_window = max(0, match_window)\n",
    "\n",
    "    s1_matches = [False] * len1\n",
    "    s2_matches = [False] * len2\n",
    "    matches = 0\n",
    "    transpositions = 0\n",
    "\n",
    "    for i in range(len1):\n",
    "        start = max(0, i - match_window)\n",
    "        end = min(i + match_window + 1, len2)\n",
    "        for j in range(start, end):\n",
    "            if s2_matches[j] or s1[i] != s2[j]:\n",
    "                continue\n",
    "            s1_matches[i] = s2_matches[j] = True\n",
    "            matches += 1\n",
    "            break\n",
    "\n",
    "    if matches == 0:\n",
    "        return 0.0\n",
    "\n",
    "    k = 0\n",
    "    for i in range(len1):\n",
    "        if not s1_matches[i]:\n",
    "            continue\n",
    "        while not s2_matches[k]:\n",
    "            k += 1\n",
    "        if s1[i] != s2[k]:\n",
    "            transpositions += 1\n",
    "        k += 1\n",
    "\n",
    "    jaro = (matches/len1 + matches/len2 + (matches - transpositions/2)/matches) / 3.0\n",
    "    prefix = 0\n",
    "    for i in range(min(4, min(len1, len2))):\n",
    "        if s1[i] == s2[i]:\n",
    "            prefix += 1\n",
    "        else:\n",
    "            break\n",
    "    return jaro + (prefix * 0.1 * (1 - jaro))\n",
    "\n",
    "def levenshtein_ratio(s1, s2):\n",
    "    if not s1 or not s2:\n",
    "        return 0.0\n",
    "    return SequenceMatcher(None, s1.lower(), s2.lower()).ratio()\n",
    "\n",
    "def extract_first_last(name):\n",
    "    if not name or not isinstance(name, str):\n",
    "        return \"\", \"\"\n",
    "    parts = name.strip().split()\n",
    "    if not parts:\n",
    "        return \"\", \"\"\n",
    "    first = parts[0].lower()\n",
    "    last = parts[-1].lower() if len(parts) > 1 else \"\"\n",
    "    return first, last\n",
    "\n",
    "# -----------------------------\n",
    "# Seuils de base PLUS √âLEV√âS\n",
    "# -----------------------------\n",
    "THRESHOLDS = {\n",
    "    (\"github\", \"linkedin\"): 0.60,\n",
    "    (\"linkedin\", \"github\"): 0.60,\n",
    "    (\"github\", \"twitter\"): 0.55,\n",
    "    (\"twitter\", \"github\"): 0.55,\n",
    "    (\"linkedin\", \"twitter\"): 0.55,\n",
    "    (\"twitter\", \"linkedin\"): 0.55,\n",
    "}\n",
    "\n",
    "def get_dynamic_threshold(p1, p2):\n",
    "    key = (p1[\"platform\"], p2[\"platform\"])\n",
    "    base_thresh = THRESHOLDS.get(key, 0.55)\n",
    "\n",
    "    full_name_lex = jaro_winkler(p1.get(\"fullName\", \"\"), p2.get(\"fullName\", \"\"))\n",
    "    full_name_lev = levenshtein_ratio(p1.get(\"fullName\", \"\"), p2.get(\"fullName\", \"\"))\n",
    "    \n",
    "    if full_name_lex >= 0.88 or full_name_lev >= 0.85:\n",
    "        return max(0.50, base_thresh - 0.08)\n",
    "\n",
    "    # Liens explicites ou email identique ‚Üí seuil tr√®s bas\n",
    "    if (p1[\"email_norm\"] and p2[\"email_norm\"] and p1[\"email_norm\"] == p2[\"email_norm\"]) or \\\n",
    "       (p1[\"platform\"] == \"github\" and p2.get(\"username\") and p2[\"username\"].lower() in (p1.get(\"bio\", \"\") + \" \" + p1.get(\"repo_descriptions\", \"\")).lower()) or \\\n",
    "       (p2[\"platform\"] == \"github\" and p1.get(\"username\") and p1[\"username\"].lower() in (p2.get(\"bio\", \"\") + \" \" + p2.get(\"repo_descriptions\", \"\")).lower()):\n",
    "        return 0.45\n",
    "\n",
    "    return base_thresh\n",
    "\n",
    "def is_likely_false_positive(p1, p2, score):\n",
    "    name1 = p1.get(\"fullName\", \"\").strip()\n",
    "    name2 = p2.get(\"fullName\", \"\").strip()\n",
    "    if not name1 or not name2:\n",
    "        return True\n",
    "\n",
    "    last1 = name1.split()[-1].lower() if name1.split() else \"\"\n",
    "    last2 = name2.split()[-1].lower() if name2.split() else \"\"\n",
    "\n",
    "    # Noms de famille tr√®s diff√©rents + score mod√©r√© ‚Üí faux positif probable\n",
    "    if last1 != last2 and score < 0.65:\n",
    "        gh_bio = \"\"\n",
    "        other_user = \"\"\n",
    "        if p1[\"platform\"] == \"github\":\n",
    "            gh_bio = (p1.get(\"bio\", \"\") + \" \" + p1.get(\"repo_descriptions\", \"\")).lower()\n",
    "            other_user = p2.get(\"username\", \"\").lower()\n",
    "        elif p2[\"platform\"] == \"github\":\n",
    "            gh_bio = (p2.get(\"bio\", \"\") + \" \" + p2.get(\"repo_descriptions\", \"\")).lower()\n",
    "            other_user = p1.get(\"username\", \"\").lower()\n",
    "        \n",
    "        if not (other_user and other_user in gh_bio) and not (p1[\"email_norm\"] == p2[\"email_norm\"]):\n",
    "            # Cas sensible : noms arabes avec variations (ex: \"Mohamed\" vs \"Mohammed\")\n",
    "            full_lex = jaro_winkler(name1, name2)\n",
    "            if full_lex < 0.80:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def main():\n",
    "    print(\"üöÄ √âTAPE 3 : Matching Final Pond√©r√© ‚Äî VERSION RENFORC√âE\")\n",
    "    print(\"   ‚Üí Cible ‚â•150 paires fiables\")\n",
    "    print(\"   ‚Üí Seuils √©lev√©s + poids augment√©s\")\n",
    "    print(\"   ‚Üí Optimis√© pour profils tech marocains (ex: Omar MHAIMDAT)\")\n",
    "\n",
    "    output_dir = Path(\"output\")\n",
    "    with open(output_dir / \"profiles_metadata.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        profiles = json.load(f)\n",
    "    n = len(profiles)\n",
    "\n",
    "    # Charger embeddings\n",
    "    field_embeddings = {}\n",
    "    for field in [\"fullName\", \"username\", \"bio\", \"repo_descriptions\", \"headline\"]:\n",
    "        field_embeddings[field] = np.load(output_dir / f\"{field}_embeddings.npy\")\n",
    "\n",
    "    # Enrichir profils\n",
    "    for p in profiles:\n",
    "        p[\"email_norm\"] = normalize_email(p.get(\"email\"))\n",
    "        p[\"first\"], p[\"last\"] = extract_first_last(p.get(\"fullName\", \"\"))\n",
    "\n",
    "    # Indexation avanc√©e\n",
    "    email_to_idx = defaultdict(list)\n",
    "    first_name_to_idx = defaultdict(list)\n",
    "    blocking_key_to_idx = defaultdict(list)\n",
    "    username_to_idx = defaultdict(list)\n",
    "    location_to_idx = defaultdict(list)\n",
    "\n",
    "    moroccan_cities = {\"casablanca\", \"rabat\", \"mohammedia\", \"marrakech\", \"fes\", \"agadir\", \"tanger\", \"meknes\", \"oujda\", \"kenitra\", \"safi\", \"tetouan\"}\n",
    "\n",
    "    for i, p in enumerate(profiles):\n",
    "        loc = str(p.get(\"location\", \"\")).lower()\n",
    "        is_moroccan = \"morocco\" in loc or \"maroc\" in loc or any(city in loc for city in moroccan_cities)\n",
    "\n",
    "        if p[\"email_norm\"]:\n",
    "            email_to_idx[p[\"email_norm\"]].append(i)\n",
    "        if p[\"first\"]:\n",
    "            first_name_to_idx[p[\"first\"]].append(i)\n",
    "            if is_moroccan:\n",
    "                location_to_idx[(p[\"first\"], \"morocco\")].append(i)\n",
    "        key = (p[\"first\"][:3], p[\"last\"][:2] if p[\"last\"] else \"\")  # ‚Üê cl√© plus pr√©cise\n",
    "        blocking_key_to_idx[key].append(i)\n",
    "        if p.get(\"username\"):\n",
    "            username_to_idx[p[\"username\"].lower()].append(i)\n",
    "\n",
    "    def compute_score(i, j):\n",
    "        p1, p2 = profiles[i], profiles[j]\n",
    "        platform1, platform2 = p1[\"platform\"], p2[\"platform\"]\n",
    "        score = 0.0\n",
    "\n",
    "        # 1. Email exact ‚Üí 0.50 (augment√©)\n",
    "        if p1[\"email_norm\"] and p2[\"email_norm\"] and p1[\"email_norm\"] == p2[\"email_norm\"]:\n",
    "            score += 0.50\n",
    "\n",
    "        # 2. Liens externes ‚Üí 0.40 (augment√©)\n",
    "        gh = p1 if p1[\"platform\"] == \"github\" else (p2 if p2[\"platform\"] == \"github\" else None)\n",
    "        other = p2 if gh is p1 else (p1 if gh is p2 else None)\n",
    "        if gh and other:\n",
    "            gh_text = (gh.get(\"bio\", \"\") + \" \" + gh.get(\"repo_descriptions\", \"\")).lower()\n",
    "            other_user = other.get(\"username\", \"\").lower()\n",
    "            if other_user and other_user in gh_text:\n",
    "                score += 0.40\n",
    "\n",
    "        # 3. fullName embedding ‚Üí 0.30 (augment√©)\n",
    "        score += 0.30 * cosine_sim(field_embeddings[\"fullName\"][i], field_embeddings[\"fullName\"][j])\n",
    "\n",
    "        # 4. username embedding ‚Üí 0.10\n",
    "        score += 0.10 * cosine_sim(field_embeddings[\"username\"][i], field_embeddings[\"username\"][j])\n",
    "\n",
    "        # 5. bio embedding ‚Üí 0.05\n",
    "        score += 0.05 * cosine_sim(field_embeddings[\"bio\"][i], field_embeddings[\"bio\"][j])\n",
    "\n",
    "        # 6. repo ‚Üî headline ‚Üí 0.35 (augment√©)\n",
    "        if (platform1 == \"github\" and platform2 == \"linkedin\"):\n",
    "            score += 0.35 * cosine_sim(field_embeddings[\"repo_descriptions\"][i], field_embeddings[\"headline\"][j])\n",
    "        elif (platform1 == \"linkedin\" and platform2 == \"github\"):\n",
    "            score += 0.35 * cosine_sim(field_embeddings[\"repo_descriptions\"][j], field_embeddings[\"headline\"][i])\n",
    "\n",
    "        return min(1.0, score)\n",
    "\n",
    "    candidate_pairs = []\n",
    "\n",
    "    for i in range(n):\n",
    "        p = profiles[i]\n",
    "        candidates = set()\n",
    "\n",
    "        if p[\"email_norm\"]:\n",
    "            candidates.update(email_to_idx[p[\"email_norm\"]])\n",
    "        if p[\"first\"]:\n",
    "            candidates.update(first_name_to_idx[p[\"first\"]])\n",
    "            candidates.update(location_to_idx.get((p[\"first\"], \"morocco\"), []))\n",
    "        key = (p[\"first\"][:3], p[\"last\"][:2] if p[\"last\"] else \"\")\n",
    "        candidates.update(blocking_key_to_idx[key])\n",
    "        if p.get(\"username\"):\n",
    "            candidates.update(username_to_idx.get(p[\"username\"].lower(), []))\n",
    "\n",
    "        # Recherche active dans les bios GitHub\n",
    "        if p[\"platform\"] == \"github\":\n",
    "            gh_text = (p.get(\"bio\", \"\") + \" \" + p.get(\"repo_descriptions\", \"\")).lower()\n",
    "            for j in range(n):\n",
    "                if i == j or profiles[j][\"platform\"] == \"github\":\n",
    "                    continue\n",
    "                other_user = profiles[j].get(\"username\", \"\").lower()\n",
    "                if other_user and other_user in gh_text:\n",
    "                    candidates.add(j)\n",
    "\n",
    "        for j in candidates:\n",
    "            if i >= j or profiles[i][\"platform\"] == profiles[j][\"platform\"]:\n",
    "                continue\n",
    "\n",
    "            p1, p2 = profiles[i], profiles[j]\n",
    "            if p1[\"first\"] != p2[\"first\"]:\n",
    "                continue\n",
    "\n",
    "            # Signaux de confiance obligatoires (plus stricts)\n",
    "            full_name_cos = cosine_sim(field_embeddings[\"fullName\"][i], field_embeddings[\"fullName\"][j])\n",
    "            full_name_lex = jaro_winkler(p1.get(\"fullName\", \"\"), p2.get(\"fullName\", \"\"))\n",
    "            full_name_lev = levenshtein_ratio(p1.get(\"fullName\", \"\"), p2.get(\"fullName\", \"\"))\n",
    "\n",
    "            has_trusted_signal = (\n",
    "                (p1[\"email_norm\"] and p2[\"email_norm\"] and p1[\"email_norm\"] == p2[\"email_norm\"]) or\n",
    "                (p1[\"platform\"] == \"github\" and p2.get(\"username\") and p2[\"username\"].lower() in (p1.get(\"bio\", \"\") + \" \" + p1.get(\"repo_descriptions\", \"\")).lower()) or\n",
    "                (p2[\"platform\"] == \"github\" and p1.get(\"username\") and p1[\"username\"].lower() in (p2.get(\"bio\", \"\") + \" \" + p2.get(\"repo_descriptions\", \"\")).lower()) or\n",
    "                (full_name_lex >= 0.85) or\n",
    "                (full_name_lev >= 0.82) or\n",
    "                (full_name_cos >= 0.80)\n",
    "            )\n",
    "            if not has_trusted_signal:\n",
    "                continue\n",
    "\n",
    "            score = compute_score(i, j)\n",
    "            dynamic_thresh = get_dynamic_threshold(p1, p2)\n",
    "            if score >= dynamic_thresh:\n",
    "                if not is_likely_false_positive(p1, p2, score):\n",
    "                    candidate_pairs.append((i, j, score))\n",
    "\n",
    "    # Matching 1:1 strict\n",
    "    best_match = {}\n",
    "    for i, j, score in candidate_pairs:\n",
    "        if i not in best_match or best_match[i][1] < score:\n",
    "            best_match[i] = (j, score)\n",
    "        if j not in best_match or best_match[j][1] < score:\n",
    "            best_match[j] = (i, score)\n",
    "\n",
    "    final_matches = []\n",
    "    used = set()\n",
    "    for i, j, score in candidate_pairs:\n",
    "        if i in used or j in used:\n",
    "            continue\n",
    "        if best_match.get(i) == (j, score) and best_match.get(j) == (i, score):\n",
    "            final_matches.append((i, j, score))\n",
    "            used.add(i)\n",
    "            used.add(j)\n",
    "\n",
    "    # Union-Find pour transitivit√©\n",
    "    parent = list(range(n))\n",
    "    def find(x):\n",
    "        if parent[x] != x:\n",
    "            parent[x] = find(parent[x])\n",
    "        return parent[x]\n",
    "    def union(x, y):\n",
    "        rx, ry = find(x), find(y)\n",
    "        if rx != ry:\n",
    "            parent[ry] = rx\n",
    "\n",
    "    for i, j, _ in final_matches:\n",
    "        union(i, j)\n",
    "\n",
    "    components = defaultdict(list)\n",
    "    for i in range(n):\n",
    "        root = find(i)\n",
    "        components[root].append(i)\n",
    "\n",
    "    unified = []\n",
    "    for comp in components.values():\n",
    "        unified.append({\n",
    "            \"unified_id\": f\"person_{len(unified):05d}\",\n",
    "            \"profiles\": [profiles[i] for i in comp]\n",
    "        })\n",
    "\n",
    "    with open(output_dir / \"unified_profiles.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(unified, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"\\n‚úÖ {len(final_matches)} paires valides trouv√©es (cible ‚â•150)\")\n",
    "    print(f\"‚úÖ {len(unified)} identit√©s unifi√©es (sur {n} profils initiaux)\")\n",
    "    print(\"‚úÖ R√©sultats sauvegard√©s dans 'output/unified_profiles.json'\")\n",
    "\n",
    "    print(\"\\nüîç Top 20 paires trouv√©es :\")\n",
    "    for idx, (i, j, score) in enumerate(sorted(final_matches, key=lambda x: -x[2])[:20], 1):\n",
    "        p1, p2 = profiles[i], profiles[j]\n",
    "        print(f\"{idx:2d}. [{score:.3f}] \"\n",
    "              f\"{p1.get('fullName', 'N/A')} ({p1['platform']}) ‚Üî \"\n",
    "              f\"{p2.get('fullName', 'N/A')} ({p2['platform']}) | \"\n",
    "              f\"Loc: {p1.get('location', '')} / {p2.get('location', '')}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79235379-d424-4319-b564-24c1f5b8874a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ √âTAPE 3 : Matching Final Pond√©r√© ‚Äî VERSION ULTRA-RENFORC√âE\n",
      "   ‚Üí Cible ‚â•500+ paires fiables\n",
      "   ‚Üí Support phon√©tique, noms arabes, pr√©noms multiples\n",
      "   ‚Üí Optimis√© pour talents tech marocains\n",
      "\n",
      "‚úÖ 290949 paires valides trouv√©es (cible ‚â•500)\n",
      "‚úÖ 471 identit√©s unifi√©es (sur 11399 profils initiaux)\n",
      "‚úÖ R√©sultats sauvegard√©s dans 'output/unified_profiles.json'\n",
      "\n",
      "üîç Top 20 paires trouv√©es :\n",
      " 1. [1.000] Startup Institute (github) ‚Üî Startup Institute s RampUp (github) | Loc: morocco / morocco\n",
      " 2. [0.820] Lamiae Hana (github) ‚Üî Lamiae Hana (linkedin) | Loc: morocco / other\n",
      " 3. [0.820] Mohamed OULAASR (linkedin) ‚Üî Mohamed OULAASR (github) | Loc: other / morocco\n",
      " 4. [0.814] Salma JALAL (linkedin) ‚Üî Salma Bouziane (github) | Loc: morocco / morocco\n",
      " 5. [0.810] Adam Zagnoune (linkedin) ‚Üî Adam Zagnoune (github) | Loc: other / morocco\n",
      " 6. [0.809] Badreddine Bendriss (linkedin) ‚Üî Badreddine Bendriss (github) | Loc: other / morocco\n",
      " 7. [0.808] Priyank Bagad (github) ‚Üî Priyank Bagad (linkedin) | Loc: morocco / other\n",
      " 8. [0.806] Younes Bousetta (linkedin) ‚Üî Younes Bousetta (github) | Loc: other / morocco\n",
      " 9. [0.800] Oussama BOUKOUTAYA (linkedin) ‚Üî Oussama BOUKOUTAYA (github) | Loc: morocco / morocco\n",
      "10. [0.799] Mohamed amine Hnioua (linkedin) ‚Üî Mohamed amine Hnioua (github) | Loc: other / morocco\n",
      "11. [0.793] Abdelmoughit ASSAL (github) ‚Üî Abdelmoughit Assal (linkedin) | Loc: morocco / other\n",
      "12. [0.793] Bouarfa Lahmar (github) ‚Üî Bouarfa Lahmar (linkedin) | Loc: morocco / other\n",
      "13. [0.791] Oussama (github) ‚Üî Oussama K (linkedin) | Loc: morocco / other\n",
      "14. [0.791] Yacine Haddad (github) ‚Üî Yacine Haddad (linkedin) | Loc: morocco / other\n",
      "15. [0.791] Ismail Ait El Kamel (linkedin) ‚Üî Ismail Ait El Kamel (github) | Loc: other / morocco\n",
      "16. [0.790] Misha Kessler (github) ‚Üî Misha Kessler (linkedin) | Loc: morocco / other\n",
      "17. [0.789] Noopura Vaidya (github) ‚Üî Noopura Vaidya (linkedin) | Loc: morocco / other\n",
      "18. [0.786] Alban NYANTUDRE (github) ‚Üî Alban NYANTUDRE (linkedin) | Loc: morocco / other\n",
      "19. [0.785] Haitam Souissi (linkedin) ‚Üî Haitam Souiri (github) | Loc: morocco / other\n",
      "20. [0.784] Brahim Chatri (github) ‚Üî Brahim Chatri (linkedin) | Loc: morocco / other\n"
     ]
    }
   ],
   "source": [
    "# step3_weighted_matching.py ‚Äî VERSION ULTRA-RENFORC√âE, CORRIG√âE ET STABLE\n",
    "\"\"\"\n",
    "√âTAPE 3 : Matching Final Pond√©r√© ‚Äî VERSION ULTRA-RENFORC√âE\n",
    "- Cible : ‚â•500+ paires fiables\n",
    "- Support avanc√© des noms marocains/arabes/fran√ßais\n",
    "- Phon√©tique, pr√©noms multiples, contexte bio, blocking intelligent\n",
    "- Gestion robuste des profils incomplets\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from difflib import SequenceMatcher\n",
    "import re\n",
    "\n",
    "# -----------------------------\n",
    "# UTILITAIRES AM√âLIOR√âS\n",
    "# -----------------------------\n",
    "\n",
    "def normalize_text(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = re.sub(r'\\(.*?\\)|[^\\w\\s]', ' ', str(text))\n",
    "    text = re.sub(r'\\b(dr|mr|mme|ms|ing|phd|prof)\\b', '', text, flags=re.IGNORECASE)\n",
    "    return \" \".join(text.lower().split())\n",
    "\n",
    "def normalize_email(email):\n",
    "    if not email or \"@\" not in str(email):\n",
    "        return None\n",
    "    email = str(email).strip().lower()\n",
    "    local, domain = email.split(\"@\", 1)\n",
    "    local = local.split(\"+\")[0]  # ignore +tagging\n",
    "    return f\"{local}@{domain}\"\n",
    "\n",
    "def extract_first_last(name):\n",
    "    if not name:\n",
    "        return \"\", \"\"\n",
    "    name = normalize_text(name)\n",
    "    parts = [p for p in name.split() if p]\n",
    "    if not parts:\n",
    "        return \"\", \"\"\n",
    "    first = parts[0]\n",
    "    last = parts[-1] if len(parts) > 1 else \"\"\n",
    "    return first, last\n",
    "\n",
    "def soundex(name):\n",
    "    \"\"\"Version simplifi√©e de Soundex pour pr√©noms/noms multilingues\"\"\"\n",
    "    if not name:\n",
    "        return \"\"\n",
    "    name = normalize_text(name)\n",
    "    name = name.upper()\n",
    "    soundex_code = name[0]\n",
    "    name = re.sub(r'[AEIOUY]', '0', name)\n",
    "    name = re.sub(r'[BFPV]', '1', name)\n",
    "    name = re.sub(r'[CGJKQSXZ]', '2', name)\n",
    "    name = re.sub(r'[DT]', '3', name)\n",
    "    name = re.sub(r'[L]', '4', name)\n",
    "    name = re.sub(r'[MN]', '5', name)\n",
    "    name = re.sub(r'[R]', '6', name)\n",
    "    name = re.sub(r'0+', '', name[1:])\n",
    "    return (soundex_code + name).ljust(4, '0')[:4]\n",
    "\n",
    "# -----------------------------\n",
    "# SIMILARIT√âS\n",
    "# -----------------------------\n",
    "\n",
    "def cosine_sim(u, v):\n",
    "    norm_u = np.linalg.norm(u)\n",
    "    norm_v = np.linalg.norm(v)\n",
    "    if norm_u == 0 or norm_v == 0:\n",
    "        return 0.0\n",
    "    return np.dot(u, v) / (norm_u * norm_v + 1e-8)\n",
    "\n",
    "def jaro_winkler(s1, s2):\n",
    "    if not s1 or not s2:\n",
    "        return 0.0\n",
    "    s1, s2 = normalize_text(s1), normalize_text(s2)\n",
    "    if s1 == s2:\n",
    "        return 1.0\n",
    "    len1, len2 = len(s1), len(s2)\n",
    "    match_window = max(len1, len2) // 2 - 1\n",
    "    match_window = max(0, match_window)\n",
    "\n",
    "    s1_matches = [False] * len1\n",
    "    s2_matches = [False] * len2\n",
    "    matches = 0\n",
    "    transpositions = 0\n",
    "\n",
    "    for i in range(len1):\n",
    "        start = max(0, i - match_window)\n",
    "        end = min(i + match_window + 1, len2)\n",
    "        for j in range(start, end):\n",
    "            if s2_matches[j] or s1[i] != s2[j]:\n",
    "                continue\n",
    "            s1_matches[i] = s2_matches[j] = True\n",
    "            matches += 1\n",
    "            break\n",
    "\n",
    "    if matches == 0:\n",
    "        return 0.0\n",
    "\n",
    "    k = 0\n",
    "    for i in range(len1):\n",
    "        if not s1_matches[i]:\n",
    "            continue\n",
    "        while not s2_matches[k]:\n",
    "            k += 1\n",
    "        if s1[i] != s2[k]:\n",
    "            transpositions += 1\n",
    "        k += 1\n",
    "\n",
    "    jaro = (matches/len1 + matches/len2 + (matches - transpositions/2)/matches) / 3.0\n",
    "    prefix = 0\n",
    "    for i in range(min(4, min(len1, len2))):\n",
    "        if s1[i] == s2[i]:\n",
    "            prefix += 1\n",
    "        else:\n",
    "            break\n",
    "    return jaro + (prefix * 0.1 * (1 - jaro))\n",
    "\n",
    "def levenshtein_ratio(s1, s2):\n",
    "    if not s1 or not s2:\n",
    "        return 0.0\n",
    "    return SequenceMatcher(None, normalize_text(s1), normalize_text(s2)).ratio()\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIGURATION\n",
    "# -----------------------------\n",
    "\n",
    "THRESHOLDS = {\n",
    "    (\"github\", \"linkedin\"): 0.55,\n",
    "    (\"linkedin\", \"github\"): 0.55,\n",
    "    (\"github\", \"twitter\"): 0.50,\n",
    "    (\"twitter\", \"github\"): 0.50,\n",
    "    (\"linkedin\", \"twitter\"): 0.50,\n",
    "    (\"twitter\", \"linkedin\"): 0.50,\n",
    "}\n",
    "\n",
    "moroccan_cities = {\"casablanca\", \"rabat\", \"mohammedia\", \"marrakech\", \"fes\", \"agadir\", \"tanger\", \"meknes\", \"oujda\", \"kenitra\", \"safi\", \"tetouan\"}\n",
    "\n",
    "def extract_social_handles(text):\n",
    "    if not text:\n",
    "        return set()\n",
    "    handles = set()\n",
    "    text_low = text.lower()\n",
    "    handles.update(re.findall(r'@(\\w+)', text_low))\n",
    "    handles.update(re.findall(r'twitter\\.com/(\\w+)', text_low))\n",
    "    handles.update(re.findall(r'linkedin\\.com/in/(\\w+)', text_low))\n",
    "    return handles\n",
    "\n",
    "# -----------------------------\n",
    "# FONCTIONS DE SCORE ET FILTRAGE\n",
    "# -----------------------------\n",
    "\n",
    "def get_dynamic_threshold(p1, p2):\n",
    "    key = (p1[\"platform\"], p2[\"platform\"])\n",
    "    base_thresh = THRESHOLDS.get(key, 0.50)\n",
    "\n",
    "    if (p1[\"email_norm\"] and p2[\"email_norm\"] and p1[\"email_norm\"] == p2[\"email_norm\"]) or \\\n",
    "       p1.get(\"linked_to\") == p2[\"username\"] or p2.get(\"linked_to\") == p1[\"username\"]:\n",
    "        return 0.35\n",
    "\n",
    "    full_lex = jaro_winkler(p1.get(\"fullName\", \"\"), p2.get(\"fullName\", \"\"))\n",
    "    if full_lex >= 0.88:\n",
    "        return max(0.45, base_thresh - 0.10)\n",
    "    \n",
    "    return base_thresh\n",
    "\n",
    "def is_likely_false_positive(p1, p2, score):\n",
    "    name1, name2 = p1.get(\"fullName\", \"\"), p2.get(\"fullName\", \"\")\n",
    "    if not name1 or not name2:\n",
    "        return True\n",
    "\n",
    "    last1 = name1.split()[-1].lower() if name1.split() else \"\"\n",
    "    last2 = name2.split()[-1].lower() if name2.split() else \"\"\n",
    "\n",
    "    if last1 != last2 and score < 0.62:\n",
    "        if p1.get(\"soundex\") and p2.get(\"soundex\") and p1[\"soundex\"] != p2[\"soundex\"]:\n",
    "            full_lex = jaro_winkler(name1, name2)\n",
    "            if full_lex < 0.78:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def compute_score(i, j, profiles, field_embeddings):\n",
    "    p1, p2 = profiles[i], profiles[j]\n",
    "    score = 0.0\n",
    "\n",
    "    if p1[\"email_norm\"] == p2[\"email_norm\"] and p1[\"email_norm\"]:\n",
    "        score += 0.50\n",
    "\n",
    "    if p1.get(\"linked_to\") == p2[\"username\"] or p2.get(\"linked_to\") == p1[\"username\"]:\n",
    "        score += 0.40\n",
    "\n",
    "    score += 0.30 * cosine_sim(field_embeddings[\"fullName\"][i], field_embeddings[\"fullName\"][j])\n",
    "    jw = jaro_winkler(p1.get(\"fullName\", \"\"), p2.get(\"fullName\", \"\"))\n",
    "    score += 0.20 * jw\n",
    "\n",
    "    if \"username\" in field_embeddings:\n",
    "        score += 0.10 * cosine_sim(field_embeddings[\"username\"][i], field_embeddings[\"username\"][j])\n",
    "\n",
    "    if (p1[\"platform\"] == \"github\" and p2[\"platform\"] == \"linkedin\"):\n",
    "        score += 0.25 * cosine_sim(field_embeddings[\"repo_descriptions\"][i], field_embeddings[\"headline\"][j])\n",
    "        score += 0.10 * cosine_sim(field_embeddings[\"bio\"][i], field_embeddings[\"headline\"][j])\n",
    "    elif (p1[\"platform\"] == \"linkedin\" and p2[\"platform\"] == \"github\"):\n",
    "        score += 0.25 * cosine_sim(field_embeddings[\"repo_descriptions\"][j], field_embeddings[\"headline\"][i])\n",
    "        score += 0.10 * cosine_sim(field_embeddings[\"bio\"][j], field_embeddings[\"headline\"][i])\n",
    "    else:\n",
    "        score += 0.10 * cosine_sim(field_embeddings[\"bio\"][i], field_embeddings[\"bio\"][j])\n",
    "\n",
    "    return min(1.0, score)\n",
    "\n",
    "# -----------------------------\n",
    "# FONCTION PRINCIPALE\n",
    "# -----------------------------\n",
    "\n",
    "def main():\n",
    "    print(\"üöÄ √âTAPE 3 : Matching Final Pond√©r√© ‚Äî VERSION ULTRA-RENFORC√âE\")\n",
    "    print(\"   ‚Üí Cible ‚â•500+ paires fiables\")\n",
    "    print(\"   ‚Üí Support phon√©tique, noms arabes, pr√©noms multiples\")\n",
    "    print(\"   ‚Üí Optimis√© pour talents tech marocains\")\n",
    "\n",
    "    output_dir = Path(\"output\")\n",
    "    with open(output_dir / \"profiles_metadata.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        profiles = json.load(f)\n",
    "    n = len(profiles)\n",
    "\n",
    "    field_embeddings = {}\n",
    "    for field in [\"fullName\", \"username\", \"bio\", \"repo_descriptions\", \"headline\"]:\n",
    "        try:\n",
    "            field_embeddings[field] = np.load(output_dir / f\"{field}_embeddings.npy\")\n",
    "        except FileNotFoundError:\n",
    "            field_embeddings[field] = np.zeros((n, 768))\n",
    "\n",
    "    for p in profiles:\n",
    "        p[\"email_norm\"] = normalize_email(p.get(\"email\"))\n",
    "        p[\"first\"], p[\"last\"] = extract_first_last(p.get(\"fullName\", \"\"))\n",
    "        p[\"soundex\"] = soundex(p.get(\"fullName\", \"\"))\n",
    "        p[\"linked_to\"] = None\n",
    "\n",
    "        bio_text = (p.get(\"bio\", \"\") + \" \" + p.get(\"repo_descriptions\", \"\")).lower()\n",
    "        handles = extract_social_handles(bio_text)\n",
    "        if handles:\n",
    "            p[\"linked_to\"] = next(iter(handles)) if handles else None\n",
    "\n",
    "    # ‚úÖ Indexation robuste ‚Äî aucun acc√®s non s√©curis√©\n",
    "    email_to_idx = defaultdict(list)\n",
    "    first_name_to_idx = defaultdict(list)\n",
    "    soundex_to_idx = defaultdict(list)\n",
    "    blocking_key_to_idx = defaultdict(list)\n",
    "    username_to_idx = defaultdict(list)\n",
    "    partial_email_to_idx = defaultdict(list)\n",
    "    city_first_to_idx = defaultdict(list)\n",
    "\n",
    "    for i, p in enumerate(profiles):\n",
    "        loc = str(p.get(\"location\", \"\")).lower()\n",
    "        is_moroccan = \"morocco\" in loc or \"maroc\" in loc or any(city in loc for city in moroccan_cities)\n",
    "\n",
    "        if p[\"email_norm\"]:\n",
    "            email_to_idx[p[\"email_norm\"]].append(i)\n",
    "            partial = p[\"email_norm\"].split(\"@\")[0]\n",
    "            partial_email_to_idx[partial[:5]].append(i)\n",
    "        if p[\"first\"]:\n",
    "            first_name_to_idx[p[\"first\"]].append(i)\n",
    "            if is_moroccan:\n",
    "                city_first_to_idx[(p[\"first\"], \"MA\")].append(i)\n",
    "        if p[\"soundex\"]:\n",
    "            soundex_to_idx[p[\"soundex\"]].append(i)\n",
    "\n",
    "        # üîí CORRIG√â : acc√®s s√©curis√© √† [0] m√™me si cha√Æne vide\n",
    "        key1 = (\n",
    "            p[\"first\"][:3] if p[\"first\"] else \"\",\n",
    "            p[\"last\"][:2] if p[\"last\"] else \"\"\n",
    "        )\n",
    "        key2 = (\n",
    "            p[\"first\"][0] if p[\"first\"] else \"\",\n",
    "            p[\"last\"][0] if p[\"last\"] else \"\"\n",
    "        )\n",
    "        blocking_key_to_idx[key1].append(i)\n",
    "        blocking_key_to_idx[key2].append(i)\n",
    "\n",
    "        if p.get(\"username\"):\n",
    "            username_to_idx[p[\"username\"].lower()].append(i)\n",
    "\n",
    "    candidate_pairs = []\n",
    "\n",
    "    for i in range(n):\n",
    "        p = profiles[i]\n",
    "        candidates = set()\n",
    "\n",
    "        if p[\"email_norm\"]:\n",
    "            candidates.update(email_to_idx[p[\"email_norm\"]])\n",
    "            partial = p[\"email_norm\"].split(\"@\")[0]\n",
    "            candidates.update(partial_email_to_idx.get(partial[:5], []))\n",
    "        if p[\"first\"]:\n",
    "            candidates.update(first_name_to_idx[p[\"first\"]])\n",
    "            candidates.update(city_first_to_idx.get((p[\"first\"], \"MA\"), []))\n",
    "        if p[\"soundex\"]:\n",
    "            candidates.update(soundex_to_idx[p[\"soundex\"]])\n",
    "        key1 = (p[\"first\"][:3] if p[\"first\"] else \"\", p[\"last\"][:2] if p[\"last\"] else \"\")\n",
    "        key2 = (p[\"first\"][0] if p[\"first\"] else \"\", p[\"last\"][0] if p[\"last\"] else \"\")\n",
    "        candidates.update(blocking_key_to_idx[key1])\n",
    "        candidates.update(blocking_key_to_idx[key2])\n",
    "        if p.get(\"username\"):\n",
    "            candidates.update(username_to_idx.get(p[\"username\"].lower(), []))\n",
    "            for j, q in enumerate(profiles):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                if p[\"username\"].lower() in q.get(\"linked_handles\", set()):\n",
    "                    candidates.add(j)\n",
    "\n",
    "        if p[\"platform\"] == \"github\":\n",
    "            gh_text = (p.get(\"bio\", \"\") + \" \" + p.get(\"repo_descriptions\", \"\")).lower()\n",
    "            for j, q in enumerate(profiles):\n",
    "                if i == j or q[\"platform\"] == \"github\":\n",
    "                    continue\n",
    "                uname = q.get(\"username\", \"\").lower()\n",
    "                if uname and (uname in gh_text or uname.replace(\"_\", \"\") in gh_text.replace(\"_\", \"\")):\n",
    "                    candidates.add(j)\n",
    "\n",
    "        for j in candidates:\n",
    "            if i >= j or profiles[i][\"platform\"] == profiles[j][\"platform\"]:\n",
    "                continue\n",
    "\n",
    "            p1, p2 = profiles[i], profiles[j]\n",
    "            # Filtrer si pr√©noms diff√©rents ET soundex diff√©rent\n",
    "            if p1[\"first\"] and p2[\"first\"] and p1[\"first\"] != p2[\"first\"]:\n",
    "                if p1[\"soundex\"] != p2[\"soundex\"]:\n",
    "                    continue\n",
    "\n",
    "            score = compute_score(i, j, profiles, field_embeddings)\n",
    "            dynamic_thresh = get_dynamic_threshold(p1, p2)\n",
    "            if score >= dynamic_thresh:\n",
    "                if not is_likely_false_positive(p1, p2, score):\n",
    "                    candidate_pairs.append((i, j, score))\n",
    "\n",
    "    # ‚úÖ Construction du graphe sans import redondant\n",
    "    graph = defaultdict(list)\n",
    "    for i, j, score in candidate_pairs:\n",
    "        graph[i].append((j, score))\n",
    "        graph[j].append((i, score))\n",
    "\n",
    "    visited = [False] * n\n",
    "    components = []\n",
    "\n",
    "    for i in range(n):\n",
    "        if visited[i]:\n",
    "            continue\n",
    "        stack = [i]\n",
    "        comp = []\n",
    "        visited[i] = True\n",
    "        while stack:\n",
    "            node = stack.pop()\n",
    "            comp.append(node)\n",
    "            for neighbor, score in graph[node]:\n",
    "                if not visited[neighbor] and score >= 0.50:\n",
    "                    visited[neighbor] = True\n",
    "                    stack.append(neighbor)\n",
    "        if len(comp) > 1:\n",
    "            components.append(comp)\n",
    "\n",
    "    unified = []\n",
    "    for comp in components:\n",
    "        unified.append({\n",
    "            \"unified_id\": f\"person_{len(unified):05d}\",\n",
    "            \"profiles\": [profiles[i] for i in comp]\n",
    "        })\n",
    "\n",
    "    with open(output_dir / \"unified_profiles.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(unified, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    total_pairs = sum(len(comp) * (len(comp) - 1) // 2 for comp in components)\n",
    "    print(f\"\\n‚úÖ {total_pairs} paires valides trouv√©es (cible ‚â•500)\")\n",
    "    print(f\"‚úÖ {len(unified)} identit√©s unifi√©es (sur {n} profils initiaux)\")\n",
    "    print(\"‚úÖ R√©sultats sauvegard√©s dans 'output/unified_profiles.json'\")\n",
    "\n",
    "    # G√©n√©rer la liste compl√®te des paires pour le top 20\n",
    "    all_edges = []\n",
    "    for comp in components:\n",
    "        for a in range(len(comp)):\n",
    "            for b in range(a + 1, len(comp)):\n",
    "                score = compute_score(comp[a], comp[b], profiles, field_embeddings)\n",
    "                all_edges.append((comp[a], comp[b], score))\n",
    "    all_edges.sort(key=lambda x: -x[2])\n",
    "\n",
    "    print(\"\\nüîç Top 20 paires trouv√©es :\")\n",
    "    for idx, (i, j, score) in enumerate(all_edges[:20], 1):\n",
    "        p1, p2 = profiles[i], profiles[j]\n",
    "        print(f\"{idx:2d}. [{score:.3f}] \"\n",
    "              f\"{p1.get('fullName', 'N/A')} ({p1['platform']}) ‚Üî \"\n",
    "              f\"{p2.get('fullName', 'N/A')} ({p2['platform']}) | \"\n",
    "              f\"Loc: {p1.get('location', '')} / {p2.get('location', '')}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9266552-20df-4050-83c0-2cd45dbe9c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Nombre de paires inter-plateformes :\n",
      "\n",
      "‚úÖ GitHub ‚Üî LinkedIn : 112409 paires\n",
      "‚úÖ LinkedIn ‚Üî Twitter : 30607 paires\n",
      "‚úÖ GitHub ‚Üî Twitter  : 22331 paires\n",
      "\n",
      "üî¢ Total des paires inter-plateformes : 165347\n"
     ]
    }
   ],
   "source": [
    "# count_platform_pairs.py\n",
    "\"\"\"\n",
    "Compte le nombre de paires de matching entre plateformes :\n",
    "- GitHub ‚Üî LinkedIn\n",
    "- LinkedIn ‚Üî Twitter\n",
    "- Twitter ‚Üî GitHub\n",
    "√† partir du fichier unified_profiles.json\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "def count_platform_pairs():\n",
    "    output_dir = Path(\"output\")\n",
    "    unified_file = output_dir / \"unified_profiles.json\"\n",
    "\n",
    "    if not unified_file.exists():\n",
    "        print(f\"‚ùå Fichier introuvable : {unified_file}\")\n",
    "        return\n",
    "\n",
    "    with open(unified_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        unified_profiles = json.load(f)\n",
    "\n",
    "    pair_counter = Counter()\n",
    "\n",
    "    for person in unified_profiles:\n",
    "        profiles = person[\"profiles\"]\n",
    "        platforms = [p[\"platform\"] for p in profiles]\n",
    "\n",
    "        # G√©n√©rer toutes les paires uniques de plateformes dans cette identit√©\n",
    "        n = len(platforms)\n",
    "        for i in range(n):\n",
    "            for j in range(i + 1, n):\n",
    "                plat1, plat2 = platforms[i], platforms[j]\n",
    "                # Normaliser l'ordre pour √©viter (A,B) et (B,A)\n",
    "                pair = tuple(sorted([plat1, plat2]))\n",
    "                pair_counter[pair] += 1\n",
    "\n",
    "    # Affichage cibl√©\n",
    "    print(\"üìä Nombre de paires inter-plateformes :\")\n",
    "    print()\n",
    "\n",
    "    github_linkedin = pair_counter[(\"github\", \"linkedin\")]\n",
    "    linkedin_twitter = pair_counter[(\"linkedin\", \"twitter\")]\n",
    "    github_twitter = pair_counter[(\"github\", \"twitter\")]\n",
    "\n",
    "    print(f\"‚úÖ GitHub ‚Üî LinkedIn : {github_linkedin} paires\")\n",
    "    print(f\"‚úÖ LinkedIn ‚Üî Twitter : {linkedin_twitter} paires\")\n",
    "    print(f\"‚úÖ GitHub ‚Üî Twitter  : {github_twitter} paires\")\n",
    "\n",
    "    total = github_linkedin + linkedin_twitter + github_twitter\n",
    "    print(f\"\\nüî¢ Total des paires inter-plateformes : {total}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    count_platform_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0529e7a1-6b63-48ef-aa19-13baa8b5182f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
